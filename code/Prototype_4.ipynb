{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prototype4v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fniv64S3PwW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training model 5000 (pure sine, same dimensions as disc groove)\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one np array as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions):\n",
        "  reshaped = []\n",
        "  for i in range(0, data.shape[0]+1-time_steps, predictions):\n",
        "    reshaped.append(data[i:i+time_steps,:])\n",
        "  return np.array(reshaped)\n",
        "\n",
        "#generate a perfect sine wave with amplitude range [-1;1] in a format ready for goal\n",
        "def generate_sine(freq, size, offset, predictions):\n",
        "  timestep = 1.0/104000.0\n",
        "  sine = []\n",
        "  block = []\n",
        "  for i in range(size):\n",
        "    amp = math.cos(2*math.pi*freq*(i+offset)*timestep)\n",
        "    block.append(amp)\n",
        "    if (i+1) % predictions == 0:\n",
        "      sine.append(block)\n",
        "      block = []\n",
        "  return np.array(sine)\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "\n",
        "#create model\n",
        "def create_model(filter_shapes, \n",
        "                 time_steps, \n",
        "                 img_width, \n",
        "                 activation_function, \n",
        "                 max_pool_shape, \n",
        "                 dropout_rate, \n",
        "                 predictions, \n",
        "                 padding,\n",
        "                 learning_rate,\n",
        "                 decay,\n",
        "                 model_dir,\n",
        "                 model_name):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, filter_shapes[0],\n",
        "                   input_shape=(time_steps, img_width, 1), padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, filter_shapes[1], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))   \n",
        "  if not dropout_rate[0]==0.0:\n",
        "    model.add(Dropout(dropout_rate[0]))\n",
        "  \n",
        "  model.add(Conv2D(64, filter_shapes[2], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, filter_shapes[3], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))\n",
        "  if not dropout_rate[1]==0.0:\n",
        "    model.add(Dropout(dropout_rate[1]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not dropout_rate[2]==0.0:\n",
        "    model.add(Dropout(dropout_rate[2]))\n",
        "  model.add(Dense(predictions))\n",
        "  model.add(Activation('tanh'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=opt,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "            \n",
        "  with open(os.path.join(model_dir, model_name+'_parameters.txt'), 'w') as f:\n",
        "    f.write('time_steps_a : '+str(time_steps)+'\\n')\n",
        "    f.write('predictions : ' + str(predictions)+'\\n')\n",
        "    f.write('filter_shapes : '+str(filter_shapes)+'\\n')\n",
        "    f.write('max_pool_shape : '+str(max_pool_shape)+'\\n')\n",
        "    f.write('dropout_rate : '+str(dropout_rate)+'\\n')\n",
        "    f.write('activation : '+activation_function+'\\n')\n",
        "    f.write('padding : '+padding+'\\n')\n",
        "    f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "    f.write('decay : '+str(decay)+'\\n')\n",
        "    f.write('optimizer : '+str(opt)+'\\n')\n",
        "  \n",
        "  model_first_save(model, model_dir, model_name)\n",
        "  return model\n",
        "  \n",
        "def model_first_save(model, model_dir, model_name):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  plot_model(model, to_file=os.path.join(model_dir, model_name+'_architecture.png'), show_shapes=True, show_layer_names=False)\n",
        "  with open(os.path.join(model_dir, model_name+'_summary.txt'), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "def fit_model(model, images, time_steps, predictions, batch_size, epoch):\n",
        "  counter=1\n",
        "  for image_name in images:\n",
        "    print('image '+str(counter)+'/'+str(len(images)))\n",
        "    #print('name : '+image_name)\n",
        "    counter+=1\n",
        "    img = io.imread(os.path.join(img_train_dir, image_name) , as_gray=True)\n",
        "    img = noising(img)\n",
        "    img = img.reshape(img.shape[0], img.shape[1], 1)\n",
        "    img_norm = normalize_u8bits(img)\n",
        "    x_train = window_reshape(img_norm, time_steps, predictions)\n",
        "\n",
        "    freq = int(image_name[1:image_name.find('_')])\n",
        "\n",
        "    y_train = generate_sine(freq, x_train.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epoch,\n",
        "                        initial_epoch=epoch-1,\n",
        "                        shuffle=True,\n",
        "                        validation_split=0.15,\n",
        "                        verbose=1)\n",
        "  return history\n",
        "  \n",
        "def test_model(model, time_steps, predictions):\n",
        "  testing = os.listdir(img_test_dir)\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    #img_test = noising(img_test)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "\n",
        "def noising(img, seed=None):\n",
        "    \n",
        "    random.seed(seed)\n",
        "    #scratches generator\n",
        "    for i in range(40000):\n",
        "        h = random.randint(1, 3)\n",
        "        w = random.randint(50, 150)\n",
        "        y = random.randint(0, img.shape[0]+1)\n",
        "        x = random.randint(0, img.shape[1]+1)\n",
        "\n",
        "        img[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "        \n",
        "    #holes generator\n",
        "    for i in range(8000):\n",
        "        h = random.randint(5, 15)\n",
        "        w = random.randint(10, 100)\n",
        "        y = random.randint(0, img.shape[0]+1)\n",
        "        x = random.randint(0, img.shape[1]+1)\n",
        "\n",
        "        img[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "        \n",
        "    np.random.seed(seed)\n",
        "    gauss = np.random.normal(-100.0,50.0,img.shape)\n",
        "    noisy = img + gauss\n",
        "    clipped = np.clip(noisy, 0, 255)\n",
        "    \n",
        "    return np.around(clipped).astype(np.uint8)\n",
        "\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "img_train_dir = \"/content/drive/My Drive/disc_sim_data/train\"\n",
        "img_test_dir = \"/content/drive/My Drive/disc_sim_data/noisy_test\"\n",
        "\n",
        "model_version_start = 5000 #for output numbering\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "epoch_start = 2\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "time_steps = 15\n",
        "predictions = 1\n",
        "filter_shapes = [(3,3),(3,3),(3,3),(3,3)]\n",
        "img_width = 320\n",
        "learning_rate = 0.001\n",
        "decay = 0.0\n",
        "steps_for_testing = 100 # 1 step = 1 image\n",
        "max_pool_shape = (2, 2)\n",
        "dropout_rate = [0.25, 0.25, 0.5]\n",
        "activation_function = 'relu'\n",
        "padding='same'\n",
        "division = 5 # epoch division for testing and saving\n",
        "\n",
        "current_epoch = 1\n",
        "current_mv = 0\n",
        "\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  \n",
        "  model_version = model_version_start\n",
        "  \n",
        "  images = os.listdir(img_train_dir)\n",
        "  shuffle(images)\n",
        "  \n",
        "  model_name = 'model'+str(model_version)\n",
        "\n",
        "  print(model_name+', epoch '+str(epoch))\n",
        "  \n",
        "  model_dir = os.path.join(models_dir, model_name)\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  #loading model\n",
        "  if epoch > 1:\n",
        "    model = load_model(os.path.join(model_dir, model_name+'_epoch'+str(epoch-1)+'.h5'))\n",
        "  else:\n",
        "\n",
        "    model = create_model(filter_shapes=filter_shapes,\n",
        "                        time_steps=time_steps,\n",
        "                        img_width=img_width,\n",
        "                        activation_function=activation_function,\n",
        "                        max_pool_shape=max_pool_shape,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        predictions=predictions,\n",
        "                        padding=padding,\n",
        "                        learning_rate = learning_rate,\n",
        "                        decay = decay,\n",
        "                        model_dir = model_dir,\n",
        "                        model_name = model_name)\n",
        "  size = len(images)//division\n",
        "  for nth in range(0, division) :\n",
        "    #fitting model over one epoch\n",
        "    train_results = fit_model(model, images[nth*size:(nth+1)*size], time_steps, predictions, batch_size, epoch)\n",
        "\n",
        "    #saving model\n",
        "    stamp = str(epoch-1)+'.'+str(nth+1) if nth < division-1 else str(epoch)\n",
        "    model.save(os.path.join(model_dir, model_name+'_epoch'+stamp+'.h5'))\n",
        "\n",
        "    #testing model\n",
        "    test_results = test_model(model, time_steps, predictions)\n",
        "\n",
        "    #appending results\n",
        "    result_dir = os.path.join(results_dir, model_name)\n",
        "    if epoch==1 and nth == 0:\n",
        "      results = {}\n",
        "      results['mae_validation']={}\n",
        "      results['mse_validation']={}\n",
        "      results['mae_test']={}\n",
        "      results['mse_test']={}\n",
        "      if not os.path.isdir(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "    else:\n",
        "      with open(os.path.join(result_dir, model_name+'_results.json'), 'r') as fp:\n",
        "        results = json.load(fp)\n",
        "\n",
        "    results['mae_validation'][stamp] = train_results.history['val_mean_absolute_error']\n",
        "    results['mse_validation'][stamp] = train_results.history['val_mean_squared_error']\n",
        "    results['mae_test'][stamp] = test_results[1]\n",
        "    results['mse_test'][stamp] = test_results[0]\n",
        "\n",
        "    with open(os.path.join(result_dir, model_name+'_results.json'), 'w') as f:\n",
        "      json.dump(results, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-S0UiAIO0kA",
        "colab_type": "code",
        "outputId": "a0958313-4af9-4e2f-dc50-b2c4cd7315ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        }
      },
      "source": [
        "#Training model 5002 (disc groove, sound from tape)\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "    \n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "\n",
        "#create model\n",
        "def create_model(filter_shapes, \n",
        "                 time_steps, \n",
        "                 img_width, \n",
        "                 activation_function, \n",
        "                 max_pool_shape, \n",
        "                 dropout_rate, \n",
        "                 predictions, \n",
        "                 padding,\n",
        "                 learning_rate,\n",
        "                 decay,\n",
        "                 model_dir,\n",
        "                 model_name):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, filter_shapes[0],\n",
        "                   input_shape=(time_steps, img_width, 1), padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, filter_shapes[1], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))   \n",
        "  if not dropout_rate[0]==0.0:\n",
        "    model.add(Dropout(dropout_rate[0]))\n",
        "  \n",
        "  model.add(Conv2D(64, filter_shapes[2], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, filter_shapes[3], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))\n",
        "  if not dropout_rate[1]==0.0:\n",
        "    model.add(Dropout(dropout_rate[1]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not dropout_rate[2]==0.0:\n",
        "    model.add(Dropout(dropout_rate[2]))\n",
        "  model.add(Dense(predictions))\n",
        "  model.add(Activation('tanh'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=opt,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "            \n",
        "  with open(os.path.join(model_dir, model_name+'_parameters.txt'), 'w') as f:\n",
        "    f.write('time_steps_a : '+str(time_steps)+'\\n')\n",
        "    f.write('predictions : ' + str(predictions)+'\\n')\n",
        "    f.write('filter_shapes : '+str(filter_shapes)+'\\n')\n",
        "    f.write('max_pool_shape : '+str(max_pool_shape)+'\\n')\n",
        "    f.write('dropout_rate : '+str(dropout_rate)+'\\n')\n",
        "    f.write('activation : '+activation_function+'\\n')\n",
        "    f.write('padding : '+padding+'\\n')\n",
        "    f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "    f.write('decay : '+str(decay)+'\\n')\n",
        "    f.write('optimizer : '+str(opt)+'\\n')\n",
        "  \n",
        "  model_first_save(model, model_dir, model_name)\n",
        "  return model\n",
        "  \n",
        "def model_first_save(model, model_dir, model_name):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  plot_model(model, to_file=os.path.join(model_dir, model_name+'_architecture.png'), show_shapes=True, show_layer_names=False)\n",
        "  with open(os.path.join(model_dir, model_name+'_summary.txt'), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "def fit_model(model, images, time_steps, predictions, batch_size, epoch):\n",
        "  counter=1  \n",
        "  for image in images:\n",
        "    print('image '+str(counter)+'/'+str(len(images)))\n",
        "    #print('name : '+image_name)\n",
        "    counter+=1\n",
        "    if not image.endswith('.tif'):\n",
        "      continue\n",
        "    #read image\n",
        "    groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "    groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "    groove = normalize_u8bits(groove)\n",
        "\n",
        "    #read track\n",
        "    track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "    root = track_data.getroot()\n",
        "    track = []\n",
        "    for x in root.findall('ArrayOfDouble'):\n",
        "      point = []\n",
        "      for d in x.findall('double'):\n",
        "        point.append(int(d.text))\n",
        "      track.append(point)  \n",
        "    track.pop(0)\n",
        "\n",
        "    #read sound\n",
        "    samplerate, sound=read(os.path.join(data_dir, image.replace('.tif', '.wav')))\n",
        "    sound = np.array(sound[0:len(track)])\n",
        "    sound = normalize_16bits(sound)\n",
        "\n",
        "    for i in range(0, len(track), 50000):\n",
        "      x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "      y_data = sound[i:min(len(track), i+50000)]\n",
        "      history = model.fit(x_data, y_data, \n",
        "                batch_size=32, \n",
        "                epochs=epoch,\n",
        "                initial_epoch=epoch-1,\n",
        "                shuffle=True,\n",
        "                validation_split=0.15,\n",
        "                verbose=1)\n",
        "  return history\n",
        "  \n",
        "def test_model(model, time_steps, predictions):\n",
        "  testing = os.listdir(img_test_dir)\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    #img_test = noising(img_test)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "data_dir = \"/content/drive/My Drive/disc_data\"\n",
        "\n",
        "\n",
        "model_version_start = 6000 #for output numbering\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "epoch_start = 1\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "time_steps = 21\n",
        "predictions = 1\n",
        "filter_shapes = [(3,3),(3,3),(3,3),(3,3)]\n",
        "img_width = 360\n",
        "window_width = 360\n",
        "learning_rate = 0.0005\n",
        "decay = 0\n",
        "steps_for_testing = 100 # 1 step = 1 image\n",
        "max_pool_shape = (2, 2)\n",
        "dropout_rate = [0.5, 0.5, 0.7]\n",
        "activation_function = 'relu'\n",
        "padding='same'\n",
        "division = 1 # epoch division for testing and saving\n",
        "\n",
        "current_epoch = 1\n",
        "current_mv = 0\n",
        "\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  \n",
        "  model_version = model_version_start\n",
        "  \n",
        "  images_raw = os.listdir(data_dir)\n",
        "  images=[]\n",
        "  for image in images_raw:\n",
        "    if image.endswith('.tif'):\n",
        "      images.append(image)\n",
        "  \n",
        "  shuffle(images)\n",
        "  model_name = 'model'+str(model_version)\n",
        "\n",
        "  print(model_name+', epoch '+str(epoch))\n",
        "  \n",
        "  model_dir = os.path.join(models_dir, model_name)\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  #loading model\n",
        "  if epoch > 1:\n",
        "    model = load_model(os.path.join(model_dir, model_name+'_epoch'+str(epoch-1)+'.h5'))\n",
        "  else:\n",
        "    model = create_model(filter_shapes=filter_shapes,\n",
        "                        time_steps=time_steps,\n",
        "                        img_width=img_width,\n",
        "                        activation_function=activation_function,\n",
        "                        max_pool_shape=max_pool_shape,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        predictions=predictions,\n",
        "                        padding=padding,\n",
        "                        learning_rate = learning_rate,\n",
        "                        decay = decay,\n",
        "                        model_dir = model_dir,\n",
        "                        model_name = model_name)\n",
        "  size = len(images)//division\n",
        "  for nth in range(0, division) :\n",
        "    #fitting model over one epoch\n",
        "    train_results = fit_model(model, images[nth*size:(nth+1)*size], time_steps, predictions, batch_size, epoch)\n",
        "\n",
        "    #saving model\n",
        "    stamp = str(epoch-1)+'.'+str(nth+1) if nth < division-1 else str(epoch)\n",
        "    model.save(os.path.join(model_dir, model_name+'_epoch'+stamp+'.h5'))\n",
        "\n",
        "    #testing model\n",
        "    #test_results = test_model(model, time_steps, predictions)\n",
        "\n",
        "    #appending results\n",
        "    result_dir = os.path.join(results_dir, model_name)\n",
        "    if epoch==1 and nth == 0:\n",
        "      results = {}\n",
        "      results['mae_validation']={}\n",
        "      results['mse_validation']={}\n",
        "      #results['mae_test']={}\n",
        "      #results['mse_test']={}\n",
        "      if not os.path.isdir(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "    else:\n",
        "      with open(os.path.join(result_dir, model_name+'_results.json'), 'r') as fp:\n",
        "        results = json.load(fp)\n",
        "\n",
        "    results['mae_validation'][stamp] = train_results.history['val_mean_absolute_error']\n",
        "    results['mse_validation'][stamp] = train_results.history['val_mean_squared_error']\n",
        "    #results['mae_test'][stamp] = test_results[1]\n",
        "    #results['mse_test'][stamp] = test_results[0]\n",
        "\n",
        "    with open(os.path.join(result_dir, model_name+'_results.json'), 'w') as f:\n",
        "      json.dump(results, f)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model5005, epoch 1\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 1/1\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1198 - mean_squared_error: 0.0227 - val_loss: 0.0140 - val_mean_absolute_error: 0.0953 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 1/1\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0948 - mean_squared_error: 0.0135 - val_loss: 0.0149 - val_mean_absolute_error: 0.1019 - val_mean_squared_error: 0.0149\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 1/1\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0451 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 1/1\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1039 - mean_squared_error: 0.0227 - val_loss: 0.1021 - val_mean_absolute_error: 0.2609 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 1/1\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1770 - mean_squared_error: 0.0499 - val_loss: 0.0277 - val_mean_absolute_error: 0.1360 - val_mean_squared_error: 0.0277\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 1/1\n",
            "42176/42500 [============================>.] - ETA: 0s - loss: 0.0164 - mean_absolute_error: 0.0878 - mean_squared_error: 0.0164"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bf1f2ec252a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mnth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m#fitting model over one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m#saving model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bf1f2ec252a2>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, images, time_steps, predictions, batch_size, epoch)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 verbose=1)\n\u001b[0m\u001b[1;32m    183\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjbU6-M6mgr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99a38e5f-eacd-429c-cb53-2c1792abb339"
      },
      "source": [
        "#Finding best learning rate for Adam\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "    \n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "\n",
        "#create model\n",
        "def create_model(filter_shapes, \n",
        "                 time_steps, \n",
        "                 img_width, \n",
        "                 activation_function, \n",
        "                 max_pool_shape, \n",
        "                 dropout_rate, \n",
        "                 predictions, \n",
        "                 padding,\n",
        "                 learning_rate,\n",
        "                 decay,\n",
        "                 model_dir,\n",
        "                 model_name):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, filter_shapes[0],\n",
        "                   input_shape=(time_steps, img_width, 1), padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, filter_shapes[1], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))   \n",
        "  if not dropout_rate[0]==0.0:\n",
        "    model.add(Dropout(dropout_rate[0]))\n",
        "  \n",
        "  model.add(Conv2D(64, filter_shapes[2], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, filter_shapes[3], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))\n",
        "  if not dropout_rate[1]==0.0:\n",
        "    model.add(Dropout(dropout_rate[1]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not dropout_rate[2]==0.0:\n",
        "    model.add(Dropout(dropout_rate[2]))\n",
        "  model.add(Dense(predictions))\n",
        "  model.add(Activation('tanh'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=opt,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "            \n",
        "  with open(os.path.join(model_dir, model_name+'_parameters.txt'), 'w') as f:\n",
        "    f.write('time_steps_a : '+str(time_steps)+'\\n')\n",
        "    f.write('predictions : ' + str(predictions)+'\\n')\n",
        "    f.write('filter_shapes : '+str(filter_shapes)+'\\n')\n",
        "    f.write('max_pool_shape : '+str(max_pool_shape)+'\\n')\n",
        "    f.write('dropout_rate : '+str(dropout_rate)+'\\n')\n",
        "    f.write('activation : '+activation_function+'\\n')\n",
        "    f.write('padding : '+padding+'\\n')\n",
        "    f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "    f.write('decay : '+str(decay)+'\\n')\n",
        "    f.write('optimizer : '+str(opt)+'\\n')\n",
        "  \n",
        "  model_first_save(model, model_dir, model_name)\n",
        "  return model\n",
        "  \n",
        "def model_first_save(model, model_dir, model_name):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  plot_model(model, to_file=os.path.join(model_dir, model_name+'_architecture.png'), show_shapes=True, show_layer_names=False)\n",
        "  with open(os.path.join(model_dir, model_name+'_summary.txt'), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "def fit_model(model, images, time_steps, predictions, batch_size, epoch):\n",
        "  counter=1  \n",
        "  for image in images:\n",
        "    print('image '+str(counter)+'/'+str(len(images)))\n",
        "    #print('name : '+image_name)\n",
        "    counter+=1\n",
        "    if not image.endswith('.tif'):\n",
        "      continue\n",
        "    #read image\n",
        "    groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "    groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "    groove = normalize_u8bits(groove)\n",
        "\n",
        "    #read track\n",
        "    track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "    root = track_data.getroot()\n",
        "    track = []\n",
        "    for x in root.findall('ArrayOfDouble'):\n",
        "      point = []\n",
        "      for d in x.findall('double'):\n",
        "        point.append(int(d.text))\n",
        "      track.append(point)  \n",
        "    track.pop(0)\n",
        "\n",
        "    #read sound\n",
        "    samplerate, sound=read(os.path.join(data_dir, image.replace('.tif', '.wav')))\n",
        "    sound = np.array(sound[0:len(track)])\n",
        "    sound = normalize_16bits(sound)\n",
        "\n",
        "    for i in range(0, len(track), 50000):\n",
        "      x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "      y_data = sound[i:min(len(track), i+50000)]\n",
        "      history = model.fit(x_data, y_data, \n",
        "                batch_size=32, \n",
        "                epochs=epoch,\n",
        "                initial_epoch=epoch-1,\n",
        "                shuffle=True,\n",
        "                validation_split=0.15,\n",
        "                verbose=1)\n",
        "  return history\n",
        "  \n",
        "def test_model(model, time_steps, predictions):\n",
        "  testing = os.listdir(img_test_dir)\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    #img_test = noising(img_test)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "data_dir = \"/content/drive/My Drive/disc_data\"\n",
        "\n",
        "\n",
        "model_version_start = 6001 #for output numbering\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "epoch_start = 4\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "time_steps = 21\n",
        "predictions = 1\n",
        "filter_shapes = [(3,3),(3,3),(3,3),(3,3)]\n",
        "img_width = 360\n",
        "window_width = 360\n",
        "learning_rate = 0.0005\n",
        "decay = 0\n",
        "steps_for_testing = 100 # 1 step = 1 image\n",
        "max_pool_shape = (2, 2)\n",
        "dropout_rate = [0.25, 0.5, 0.5]\n",
        "activation_function = 'relu'\n",
        "padding='same'\n",
        "division = 1 # epoch division for testing and saving\n",
        "\n",
        "current_epoch = 1\n",
        "current_mv = 0\n",
        "\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  \n",
        "  model_version = model_version_start\n",
        "  \n",
        "  images_raw = os.listdir(data_dir)\n",
        "  images=[]\n",
        "  for image in images_raw:\n",
        "    if image.endswith('.tif'):\n",
        "      images.append(image)\n",
        "  \n",
        "  shuffle(images)\n",
        "  \n",
        "  for learning_rate in [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]:\n",
        "    \n",
        "    model_name = 'model'+str(model_version)\n",
        "\n",
        "    print(model_name+', epoch '+str(epoch))\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_name)\n",
        "    if not os.path.isdir(model_dir):\n",
        "      os.makedirs(model_dir)\n",
        "\n",
        "    #loading model\n",
        "    if epoch > 1:\n",
        "      model = load_model(os.path.join(model_dir, model_name+'_epoch'+str(epoch-1)+'.h5'))\n",
        "    else:\n",
        "      model = create_model(filter_shapes=filter_shapes,\n",
        "                          time_steps=time_steps,\n",
        "                          img_width=img_width,\n",
        "                          activation_function=activation_function,\n",
        "                          max_pool_shape=max_pool_shape,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          predictions=predictions,\n",
        "                          padding=padding,\n",
        "                          learning_rate = learning_rate,\n",
        "                          decay = decay,\n",
        "                          model_dir = model_dir,\n",
        "                          model_name = model_name)\n",
        "    size = len(images)//division\n",
        "    for nth in range(0, division) :\n",
        "      #fitting model over one epoch\n",
        "      train_results = fit_model(model, images[nth*size:(nth+1)*size], time_steps, predictions, batch_size, epoch)\n",
        "\n",
        "      #saving model\n",
        "      stamp = str(epoch-1)+'.'+str(nth+1) if nth < division-1 else str(epoch)\n",
        "      model.save(os.path.join(model_dir, model_name+'_epoch'+stamp+'.h5'))\n",
        "\n",
        "      #testing model\n",
        "      #test_results = test_model(model, time_steps, predictions)\n",
        "\n",
        "      #appending results\n",
        "      result_dir = os.path.join(results_dir, model_name)\n",
        "      if epoch==1 and nth == 0:\n",
        "        results = {}\n",
        "        results['mae_validation']={}\n",
        "        results['mse_validation']={}\n",
        "        #results['mae_test']={}\n",
        "        #results['mse_test']={}\n",
        "        if not os.path.isdir(result_dir):\n",
        "          os.makedirs(result_dir)\n",
        "      else:\n",
        "        with open(os.path.join(result_dir, model_name+'_results.json'), 'r') as fp:\n",
        "          results = json.load(fp)\n",
        "\n",
        "      results['mae_validation'][stamp] = train_results.history['val_mean_absolute_error']\n",
        "      results['mse_validation'][stamp] = train_results.history['val_mean_squared_error']\n",
        "      #results['mae_test'][stamp] = test_results[1]\n",
        "      #results['mse_test'][stamp] = test_results[0]\n",
        "\n",
        "      with open(os.path.join(result_dir, model_name+'_results.json'), 'w') as f:\n",
        "        json.dump(results, f)\n",
        "    model_version += 1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model6001, epoch 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0809 07:01:44.047894 140128243148672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0809 07:01:44.105293 140128243148672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0809 07:01:44.175969 140128243148672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0809 07:01:44.184390 140128243148672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0809 07:01:44.190922 140128243148672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0809 07:01:44.206107 140128243148672 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0809 07:01:44.599675 140128243148672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0809 07:01:48.027044 140128243148672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 88s 2ms/step - loss: 0.0249 - mean_absolute_error: 0.1265 - mean_squared_error: 0.0249 - val_loss: 0.0141 - val_mean_absolute_error: 0.0955 - val_mean_squared_error: 0.0141\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0166 - mean_absolute_error: 0.1042 - mean_squared_error: 0.0166 - val_loss: 0.0149 - val_mean_absolute_error: 0.1015 - val_mean_squared_error: 0.0149\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0815 - mean_squared_error: 0.0105 - val_loss: 0.0029 - val_mean_absolute_error: 0.0452 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0263 - mean_absolute_error: 0.1156 - mean_squared_error: 0.0263 - val_loss: 0.1022 - val_mean_absolute_error: 0.2608 - val_mean_squared_error: 0.1022\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0533 - mean_absolute_error: 0.1832 - mean_squared_error: 0.0533 - val_loss: 0.0278 - val_mean_absolute_error: 0.1352 - val_mean_squared_error: 0.0278\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0200 - mean_absolute_error: 0.1027 - mean_squared_error: 0.0200 - val_loss: 0.0610 - val_mean_absolute_error: 0.1906 - val_mean_squared_error: 0.0610\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0494 - mean_absolute_error: 0.1616 - mean_squared_error: 0.0494 - val_loss: 0.0625 - val_mean_absolute_error: 0.1911 - val_mean_squared_error: 0.0625\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0520 - mean_absolute_error: 0.1807 - mean_squared_error: 0.0520 - val_loss: 0.0317 - val_mean_absolute_error: 0.1464 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0250 - mean_absolute_error: 0.1285 - mean_squared_error: 0.0250 - val_loss: 0.0103 - val_mean_absolute_error: 0.0825 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0102 - mean_absolute_error: 0.0802 - mean_squared_error: 0.0102 - val_loss: 0.0120 - val_mean_absolute_error: 0.0972 - val_mean_squared_error: 0.0120\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0413 - mean_absolute_error: 0.1509 - mean_squared_error: 0.0413 - val_loss: 0.0372 - val_mean_absolute_error: 0.1590 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 4/4\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0535 - mean_absolute_error: 0.1828 - mean_squared_error: 0.0535 - val_loss: 0.0361 - val_mean_absolute_error: 0.1539 - val_mean_squared_error: 0.0361\n",
            "model6002, epoch 4\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0220 - mean_absolute_error: 0.1192 - mean_squared_error: 0.0220 - val_loss: 0.0140 - val_mean_absolute_error: 0.0952 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0139 - mean_absolute_error: 0.0959 - mean_squared_error: 0.0139 - val_loss: 0.0149 - val_mean_absolute_error: 0.1020 - val_mean_squared_error: 0.0149\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0071 - mean_absolute_error: 0.0672 - mean_squared_error: 0.0071 - val_loss: 0.0029 - val_mean_absolute_error: 0.0451 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0231 - mean_absolute_error: 0.1051 - mean_squared_error: 0.0231 - val_loss: 0.1021 - val_mean_absolute_error: 0.2609 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0503 - mean_absolute_error: 0.1776 - mean_squared_error: 0.0503 - val_loss: 0.0275 - val_mean_absolute_error: 0.1349 - val_mean_squared_error: 0.0275\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 82s 2ms/step - loss: 0.0167 - mean_absolute_error: 0.0892 - mean_squared_error: 0.0167 - val_loss: 0.0609 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0463 - mean_absolute_error: 0.1532 - mean_squared_error: 0.0463 - val_loss: 0.0627 - val_mean_absolute_error: 0.1908 - val_mean_squared_error: 0.0627\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0485 - mean_absolute_error: 0.1745 - mean_squared_error: 0.0485 - val_loss: 0.0317 - val_mean_absolute_error: 0.1461 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1207 - mean_squared_error: 0.0216 - val_loss: 0.0103 - val_mean_absolute_error: 0.0824 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0069 - mean_absolute_error: 0.0655 - mean_squared_error: 0.0069 - val_loss: 0.0119 - val_mean_absolute_error: 0.0967 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0380 - mean_absolute_error: 0.1427 - mean_squared_error: 0.0380 - val_loss: 0.0371 - val_mean_absolute_error: 0.1590 - val_mean_squared_error: 0.0371\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 4/4\n",
            "24828/24828 [==============================] - 48s 2ms/step - loss: 0.0501 - mean_absolute_error: 0.1768 - mean_squared_error: 0.0501 - val_loss: 0.0359 - val_mean_absolute_error: 0.1514 - val_mean_squared_error: 0.0359\n",
            "model6003, epoch 4\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0216 - val_loss: 0.0140 - val_mean_absolute_error: 0.0953 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0947 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1022 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0067 - mean_absolute_error: 0.0653 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0451 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1038 - mean_squared_error: 0.0227 - val_loss: 0.1021 - val_mean_absolute_error: 0.2609 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1770 - mean_squared_error: 0.0499 - val_loss: 0.0277 - val_mean_absolute_error: 0.1353 - val_mean_squared_error: 0.0277\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0163 - mean_absolute_error: 0.0877 - mean_squared_error: 0.0163 - val_loss: 0.0609 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0460 - mean_absolute_error: 0.1523 - mean_squared_error: 0.0460 - val_loss: 0.0628 - val_mean_absolute_error: 0.1908 - val_mean_squared_error: 0.0628\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1740 - mean_squared_error: 0.0482 - val_loss: 0.0317 - val_mean_absolute_error: 0.1461 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0214 - mean_absolute_error: 0.1201 - mean_squared_error: 0.0214 - val_loss: 0.0103 - val_mean_absolute_error: 0.0824 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0640 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0964 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0377 - mean_absolute_error: 0.1417 - mean_squared_error: 0.0377 - val_loss: 0.0372 - val_mean_absolute_error: 0.1590 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 4/4\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1764 - mean_squared_error: 0.0499 - val_loss: 0.0372 - val_mean_absolute_error: 0.1529 - val_mean_squared_error: 0.0372\n",
            "model6004, epoch 4\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0217 - mean_absolute_error: 0.1183 - mean_squared_error: 0.0217 - val_loss: 0.0140 - val_mean_absolute_error: 0.0952 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0944 - mean_squared_error: 0.0134 - val_loss: 0.0146 - val_mean_absolute_error: 0.1004 - val_mean_squared_error: 0.0146\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0065 - mean_absolute_error: 0.0644 - mean_squared_error: 0.0065 - val_loss: 0.0030 - val_mean_absolute_error: 0.0453 - val_mean_squared_error: 0.0030\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0222 - mean_absolute_error: 0.1022 - mean_squared_error: 0.0222 - val_loss: 0.1041 - val_mean_absolute_error: 0.2640 - val_mean_squared_error: 0.1041\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0495 - mean_absolute_error: 0.1761 - mean_squared_error: 0.0495 - val_loss: 0.0284 - val_mean_absolute_error: 0.1380 - val_mean_squared_error: 0.0284\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0133 - mean_absolute_error: 0.0807 - mean_squared_error: 0.0133 - val_loss: 0.0853 - val_mean_absolute_error: 0.2267 - val_mean_squared_error: 0.0853\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0452 - mean_absolute_error: 0.1512 - mean_squared_error: 0.0452 - val_loss: 0.0581 - val_mean_absolute_error: 0.1813 - val_mean_squared_error: 0.0581\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0463 - mean_absolute_error: 0.1701 - mean_squared_error: 0.0463 - val_loss: 0.0319 - val_mean_absolute_error: 0.1452 - val_mean_squared_error: 0.0319\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0211 - mean_absolute_error: 0.1191 - mean_squared_error: 0.0211 - val_loss: 0.0105 - val_mean_absolute_error: 0.0840 - val_mean_squared_error: 0.0105\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0066 - val_loss: 0.0113 - val_mean_absolute_error: 0.0936 - val_mean_squared_error: 0.0113\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0363 - mean_absolute_error: 0.1392 - mean_squared_error: 0.0363 - val_loss: 0.0382 - val_mean_absolute_error: 0.1601 - val_mean_squared_error: 0.0382\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 4/4\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0478 - mean_absolute_error: 0.1719 - mean_squared_error: 0.0478 - val_loss: 0.0489 - val_mean_absolute_error: 0.1794 - val_mean_squared_error: 0.0489\n",
            "model6005, epoch 4\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0211 - mean_absolute_error: 0.1166 - mean_squared_error: 0.0211 - val_loss: 0.0143 - val_mean_absolute_error: 0.0964 - val_mean_squared_error: 0.0143\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0126 - mean_absolute_error: 0.0909 - mean_squared_error: 0.0126 - val_loss: 0.0141 - val_mean_absolute_error: 0.0986 - val_mean_squared_error: 0.0141\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0060 - mean_absolute_error: 0.0617 - mean_squared_error: 0.0060 - val_loss: 0.0031 - val_mean_absolute_error: 0.0467 - val_mean_squared_error: 0.0031\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0188 - mean_absolute_error: 0.0943 - mean_squared_error: 0.0188 - val_loss: 0.1111 - val_mean_absolute_error: 0.2700 - val_mean_squared_error: 0.1111\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0438 - mean_absolute_error: 0.1646 - mean_squared_error: 0.0438 - val_loss: 0.0308 - val_mean_absolute_error: 0.1426 - val_mean_squared_error: 0.0308\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0114 - mean_absolute_error: 0.0757 - mean_squared_error: 0.0114 - val_loss: 0.0868 - val_mean_absolute_error: 0.2317 - val_mean_squared_error: 0.0868\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0396 - mean_absolute_error: 0.1426 - mean_squared_error: 0.0396 - val_loss: 0.0586 - val_mean_absolute_error: 0.1836 - val_mean_squared_error: 0.0586\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0397 - mean_absolute_error: 0.1555 - mean_squared_error: 0.0397 - val_loss: 0.0386 - val_mean_absolute_error: 0.1593 - val_mean_squared_error: 0.0386\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0187 - mean_absolute_error: 0.1100 - mean_squared_error: 0.0187 - val_loss: 0.0125 - val_mean_absolute_error: 0.0915 - val_mean_squared_error: 0.0125\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0061 - mean_absolute_error: 0.0610 - mean_squared_error: 0.0061 - val_loss: 0.0100 - val_mean_absolute_error: 0.0829 - val_mean_squared_error: 0.0100\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0291 - mean_absolute_error: 0.1240 - mean_squared_error: 0.0291 - val_loss: 0.0411 - val_mean_absolute_error: 0.1654 - val_mean_squared_error: 0.0411\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 4/4\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0390 - mean_absolute_error: 0.1535 - mean_squared_error: 0.0390 - val_loss: 0.0449 - val_mean_absolute_error: 0.1694 - val_mean_squared_error: 0.0449\n",
            "model6006, epoch 4\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0216 - val_loss: 0.0140 - val_mean_absolute_error: 0.0953 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0948 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1021 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0450 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1038 - mean_squared_error: 0.0227 - val_loss: 0.1022 - val_mean_absolute_error: 0.2620 - val_mean_squared_error: 0.1022\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1770 - mean_squared_error: 0.0499 - val_loss: 0.0277 - val_mean_absolute_error: 0.1356 - val_mean_squared_error: 0.0277\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0164 - mean_absolute_error: 0.0878 - mean_squared_error: 0.0164 - val_loss: 0.0609 - val_mean_absolute_error: 0.1901 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0460 - mean_absolute_error: 0.1522 - mean_squared_error: 0.0460 - val_loss: 0.0629 - val_mean_absolute_error: 0.1907 - val_mean_squared_error: 0.0629\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1740 - mean_squared_error: 0.0482 - val_loss: 0.0317 - val_mean_absolute_error: 0.1460 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 83s 2ms/step - loss: 0.0214 - mean_absolute_error: 0.1201 - mean_squared_error: 0.0214 - val_loss: 0.0103 - val_mean_absolute_error: 0.0824 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0971 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 4/4\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0377 - mean_absolute_error: 0.1418 - mean_squared_error: 0.0377 - val_loss: 0.0372 - val_mean_absolute_error: 0.1589 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 4/4\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0500 - mean_absolute_error: 0.1767 - mean_squared_error: 0.0500 - val_loss: 0.0353 - val_mean_absolute_error: 0.1515 - val_mean_squared_error: 0.0353\n",
            "model6001, epoch 5\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0246 - mean_absolute_error: 0.1258 - mean_squared_error: 0.0246 - val_loss: 0.0140 - val_mean_absolute_error: 0.0955 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0164 - mean_absolute_error: 0.1035 - mean_squared_error: 0.0164 - val_loss: 0.0149 - val_mean_absolute_error: 0.1015 - val_mean_squared_error: 0.0149\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0099 - mean_absolute_error: 0.0793 - mean_squared_error: 0.0099 - val_loss: 0.0029 - val_mean_absolute_error: 0.0452 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0258 - mean_absolute_error: 0.1140 - mean_squared_error: 0.0258 - val_loss: 0.1022 - val_mean_absolute_error: 0.2608 - val_mean_squared_error: 0.1022\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0531 - mean_absolute_error: 0.1825 - mean_squared_error: 0.0531 - val_loss: 0.0278 - val_mean_absolute_error: 0.1352 - val_mean_squared_error: 0.0278\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0195 - mean_absolute_error: 0.1013 - mean_squared_error: 0.0195 - val_loss: 0.0610 - val_mean_absolute_error: 0.1906 - val_mean_squared_error: 0.0610\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0491 - mean_absolute_error: 0.1607 - mean_squared_error: 0.0491 - val_loss: 0.0626 - val_mean_absolute_error: 0.1913 - val_mean_squared_error: 0.0626\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0514 - mean_absolute_error: 0.1793 - mean_squared_error: 0.0514 - val_loss: 0.0317 - val_mean_absolute_error: 0.1464 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0245 - mean_absolute_error: 0.1275 - mean_squared_error: 0.0245 - val_loss: 0.0103 - val_mean_absolute_error: 0.0825 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0097 - mean_absolute_error: 0.0782 - mean_squared_error: 0.0097 - val_loss: 0.0120 - val_mean_absolute_error: 0.0972 - val_mean_squared_error: 0.0120\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0409 - mean_absolute_error: 0.1497 - mean_squared_error: 0.0409 - val_loss: 0.0372 - val_mean_absolute_error: 0.1591 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 5/5\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0528 - mean_absolute_error: 0.1815 - mean_squared_error: 0.0528 - val_loss: 0.0361 - val_mean_absolute_error: 0.1538 - val_mean_squared_error: 0.0361\n",
            "model6002, epoch 5\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0218 - mean_absolute_error: 0.1187 - mean_squared_error: 0.0218 - val_loss: 0.0140 - val_mean_absolute_error: 0.0952 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0137 - mean_absolute_error: 0.0954 - mean_squared_error: 0.0137 - val_loss: 0.0150 - val_mean_absolute_error: 0.1021 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0069 - mean_absolute_error: 0.0663 - mean_squared_error: 0.0069 - val_loss: 0.0029 - val_mean_absolute_error: 0.0452 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0229 - mean_absolute_error: 0.1044 - mean_squared_error: 0.0229 - val_loss: 0.1021 - val_mean_absolute_error: 0.2608 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0502 - mean_absolute_error: 0.1774 - mean_squared_error: 0.0502 - val_loss: 0.0276 - val_mean_absolute_error: 0.1350 - val_mean_squared_error: 0.0276\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0166 - mean_absolute_error: 0.0888 - mean_squared_error: 0.0166 - val_loss: 0.0609 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0462 - mean_absolute_error: 0.1528 - mean_squared_error: 0.0462 - val_loss: 0.0628 - val_mean_absolute_error: 0.1910 - val_mean_squared_error: 0.0628\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0484 - mean_absolute_error: 0.1743 - mean_squared_error: 0.0484 - val_loss: 0.0317 - val_mean_absolute_error: 0.1462 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1206 - mean_squared_error: 0.0216 - val_loss: 0.0103 - val_mean_absolute_error: 0.0824 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0068 - mean_absolute_error: 0.0649 - mean_squared_error: 0.0068 - val_loss: 0.0119 - val_mean_absolute_error: 0.0966 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0378 - mean_absolute_error: 0.1423 - mean_squared_error: 0.0378 - val_loss: 0.0372 - val_mean_absolute_error: 0.1590 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 5/5\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0501 - mean_absolute_error: 0.1767 - mean_squared_error: 0.0501 - val_loss: 0.0358 - val_mean_absolute_error: 0.1512 - val_mean_squared_error: 0.0358\n",
            "model6003, epoch 5\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0216 - val_loss: 0.0140 - val_mean_absolute_error: 0.0953 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0947 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1021 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0451 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1038 - mean_squared_error: 0.0227 - val_loss: 0.1021 - val_mean_absolute_error: 0.2609 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1769 - mean_squared_error: 0.0499 - val_loss: 0.0276 - val_mean_absolute_error: 0.1352 - val_mean_squared_error: 0.0276\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0163 - mean_absolute_error: 0.0876 - mean_squared_error: 0.0163 - val_loss: 0.0609 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0460 - mean_absolute_error: 0.1523 - mean_squared_error: 0.0460 - val_loss: 0.0627 - val_mean_absolute_error: 0.1908 - val_mean_squared_error: 0.0627\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1740 - mean_squared_error: 0.0482 - val_loss: 0.0317 - val_mean_absolute_error: 0.1461 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0214 - mean_absolute_error: 0.1201 - mean_squared_error: 0.0214 - val_loss: 0.0103 - val_mean_absolute_error: 0.0825 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0964 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0376 - mean_absolute_error: 0.1417 - mean_squared_error: 0.0376 - val_loss: 0.0372 - val_mean_absolute_error: 0.1590 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 5/5\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1762 - mean_squared_error: 0.0499 - val_loss: 0.0374 - val_mean_absolute_error: 0.1534 - val_mean_squared_error: 0.0374\n",
            "model6004, epoch 5\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0217 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0217 - val_loss: 0.0140 - val_mean_absolute_error: 0.0952 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0134 - mean_absolute_error: 0.0941 - mean_squared_error: 0.0134 - val_loss: 0.0143 - val_mean_absolute_error: 0.0992 - val_mean_squared_error: 0.0143\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0064 - mean_absolute_error: 0.0641 - mean_squared_error: 0.0064 - val_loss: 0.0030 - val_mean_absolute_error: 0.0454 - val_mean_squared_error: 0.0030\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0220 - mean_absolute_error: 0.1015 - mean_squared_error: 0.0220 - val_loss: 0.1039 - val_mean_absolute_error: 0.2635 - val_mean_squared_error: 0.1039\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0491 - mean_absolute_error: 0.1756 - mean_squared_error: 0.0491 - val_loss: 0.0279 - val_mean_absolute_error: 0.1368 - val_mean_squared_error: 0.0279\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0132 - mean_absolute_error: 0.0803 - mean_squared_error: 0.0132 - val_loss: 0.0894 - val_mean_absolute_error: 0.2321 - val_mean_squared_error: 0.0894\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0449 - mean_absolute_error: 0.1507 - mean_squared_error: 0.0449 - val_loss: 0.0579 - val_mean_absolute_error: 0.1804 - val_mean_squared_error: 0.0579\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0460 - mean_absolute_error: 0.1697 - mean_squared_error: 0.0460 - val_loss: 0.0322 - val_mean_absolute_error: 0.1459 - val_mean_squared_error: 0.0322\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0209 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0209 - val_loss: 0.0107 - val_mean_absolute_error: 0.0848 - val_mean_squared_error: 0.0107\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0638 - mean_squared_error: 0.0066 - val_loss: 0.0113 - val_mean_absolute_error: 0.0928 - val_mean_squared_error: 0.0113\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0359 - mean_absolute_error: 0.1383 - mean_squared_error: 0.0359 - val_loss: 0.0378 - val_mean_absolute_error: 0.1591 - val_mean_squared_error: 0.0378\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 5/5\n",
            "24828/24828 [==============================] - 50s 2ms/step - loss: 0.0472 - mean_absolute_error: 0.1707 - mean_squared_error: 0.0472 - val_loss: 0.0539 - val_mean_absolute_error: 0.1874 - val_mean_squared_error: 0.0539\n",
            "model6005, epoch 5\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0208 - mean_absolute_error: 0.1155 - mean_squared_error: 0.0208 - val_loss: 0.0145 - val_mean_absolute_error: 0.0974 - val_mean_squared_error: 0.0145\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0123 - mean_absolute_error: 0.0895 - mean_squared_error: 0.0123 - val_loss: 0.0142 - val_mean_absolute_error: 0.0982 - val_mean_squared_error: 0.0142\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0059 - mean_absolute_error: 0.0612 - mean_squared_error: 0.0059 - val_loss: 0.0030 - val_mean_absolute_error: 0.0458 - val_mean_squared_error: 0.0030\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0176 - mean_absolute_error: 0.0916 - mean_squared_error: 0.0176 - val_loss: 0.1104 - val_mean_absolute_error: 0.2696 - val_mean_squared_error: 0.1104\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0414 - mean_absolute_error: 0.1594 - mean_squared_error: 0.0414 - val_loss: 0.0298 - val_mean_absolute_error: 0.1398 - val_mean_squared_error: 0.0298\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0111 - mean_absolute_error: 0.0752 - mean_squared_error: 0.0111 - val_loss: 0.0846 - val_mean_absolute_error: 0.2277 - val_mean_squared_error: 0.0846\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0371 - mean_absolute_error: 0.1386 - mean_squared_error: 0.0371 - val_loss: 0.0610 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0610\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0374 - mean_absolute_error: 0.1509 - mean_squared_error: 0.0374 - val_loss: 0.0384 - val_mean_absolute_error: 0.1586 - val_mean_squared_error: 0.0384\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0182 - mean_absolute_error: 0.1080 - mean_squared_error: 0.0182 - val_loss: 0.0131 - val_mean_absolute_error: 0.0932 - val_mean_squared_error: 0.0131\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0060 - mean_absolute_error: 0.0605 - mean_squared_error: 0.0060 - val_loss: 0.0103 - val_mean_absolute_error: 0.0862 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0271 - mean_absolute_error: 0.1196 - mean_squared_error: 0.0271 - val_loss: 0.0406 - val_mean_absolute_error: 0.1643 - val_mean_squared_error: 0.0406\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 5/5\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0368 - mean_absolute_error: 0.1491 - mean_squared_error: 0.0368 - val_loss: 0.0420 - val_mean_absolute_error: 0.1620 - val_mean_squared_error: 0.0420\n",
            "model6006, epoch 5\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0216 - val_loss: 0.0140 - val_mean_absolute_error: 0.0953 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0947 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1023 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0453 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1039 - mean_squared_error: 0.0227 - val_loss: 0.1021 - val_mean_absolute_error: 0.2614 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1770 - mean_squared_error: 0.0499 - val_loss: 0.0277 - val_mean_absolute_error: 0.1356 - val_mean_squared_error: 0.0277\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0164 - mean_absolute_error: 0.0878 - mean_squared_error: 0.0164 - val_loss: 0.0609 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0460 - mean_absolute_error: 0.1522 - mean_squared_error: 0.0460 - val_loss: 0.0629 - val_mean_absolute_error: 0.1917 - val_mean_squared_error: 0.0629\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1740 - mean_squared_error: 0.0482 - val_loss: 0.0317 - val_mean_absolute_error: 0.1462 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0214 - mean_absolute_error: 0.1201 - mean_squared_error: 0.0214 - val_loss: 0.0103 - val_mean_absolute_error: 0.0825 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0962 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 5/5\n",
            "42500/42500 [==============================] - 84s 2ms/step - loss: 0.0377 - mean_absolute_error: 0.1418 - mean_squared_error: 0.0377 - val_loss: 0.0372 - val_mean_absolute_error: 0.1589 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 5/5\n",
            "24828/24828 [==============================] - 49s 2ms/step - loss: 0.0500 - mean_absolute_error: 0.1767 - mean_squared_error: 0.0500 - val_loss: 0.0353 - val_mean_absolute_error: 0.1518 - val_mean_squared_error: 0.0353\n",
            "model6001, epoch 6\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0243 - mean_absolute_error: 0.1251 - mean_squared_error: 0.0243 - val_loss: 0.0140 - val_mean_absolute_error: 0.0954 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0159 - mean_absolute_error: 0.1023 - mean_squared_error: 0.0159 - val_loss: 0.0149 - val_mean_absolute_error: 0.1016 - val_mean_squared_error: 0.0149\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0095 - mean_absolute_error: 0.0777 - mean_squared_error: 0.0095 - val_loss: 0.0029 - val_mean_absolute_error: 0.0453 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0255 - mean_absolute_error: 0.1131 - mean_squared_error: 0.0255 - val_loss: 0.1022 - val_mean_absolute_error: 0.2607 - val_mean_squared_error: 0.1022\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0527 - mean_absolute_error: 0.1818 - mean_squared_error: 0.0527 - val_loss: 0.0278 - val_mean_absolute_error: 0.1351 - val_mean_squared_error: 0.0278\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0191 - mean_absolute_error: 0.0997 - mean_squared_error: 0.0191 - val_loss: 0.0609 - val_mean_absolute_error: 0.1905 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0485 - mean_absolute_error: 0.1593 - mean_squared_error: 0.0485 - val_loss: 0.0626 - val_mean_absolute_error: 0.1913 - val_mean_squared_error: 0.0626\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0511 - mean_absolute_error: 0.1792 - mean_squared_error: 0.0511 - val_loss: 0.0317 - val_mean_absolute_error: 0.1464 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0243 - mean_absolute_error: 0.1271 - mean_squared_error: 0.0243 - val_loss: 0.0103 - val_mean_absolute_error: 0.0825 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0094 - mean_absolute_error: 0.0770 - mean_squared_error: 0.0094 - val_loss: 0.0120 - val_mean_absolute_error: 0.0971 - val_mean_squared_error: 0.0120\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0403 - mean_absolute_error: 0.1487 - mean_squared_error: 0.0403 - val_loss: 0.0372 - val_mean_absolute_error: 0.1591 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 6/6\n",
            "24828/24828 [==============================] - 51s 2ms/step - loss: 0.0525 - mean_absolute_error: 0.1815 - mean_squared_error: 0.0525 - val_loss: 0.0361 - val_mean_absolute_error: 0.1537 - val_mean_squared_error: 0.0361\n",
            "model6002, epoch 6\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 88s 2ms/step - loss: 0.0218 - mean_absolute_error: 0.1185 - mean_squared_error: 0.0218 - val_loss: 0.0140 - val_mean_absolute_error: 0.0952 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0951 - mean_squared_error: 0.0136 - val_loss: 0.0150 - val_mean_absolute_error: 0.1021 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0068 - mean_absolute_error: 0.0659 - mean_squared_error: 0.0068 - val_loss: 0.0029 - val_mean_absolute_error: 0.0452 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0228 - mean_absolute_error: 0.1043 - mean_squared_error: 0.0228 - val_loss: 0.1021 - val_mean_absolute_error: 0.2608 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0500 - mean_absolute_error: 0.1772 - mean_squared_error: 0.0500 - val_loss: 0.0276 - val_mean_absolute_error: 0.1351 - val_mean_squared_error: 0.0276\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0165 - mean_absolute_error: 0.0884 - mean_squared_error: 0.0165 - val_loss: 0.0609 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0461 - mean_absolute_error: 0.1526 - mean_squared_error: 0.0461 - val_loss: 0.0628 - val_mean_absolute_error: 0.1911 - val_mean_squared_error: 0.0628\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0483 - mean_absolute_error: 0.1742 - mean_squared_error: 0.0483 - val_loss: 0.0317 - val_mean_absolute_error: 0.1462 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0215 - mean_absolute_error: 0.1204 - mean_squared_error: 0.0215 - val_loss: 0.0103 - val_mean_absolute_error: 0.0824 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0068 - mean_absolute_error: 0.0647 - mean_squared_error: 0.0068 - val_loss: 0.0119 - val_mean_absolute_error: 0.0966 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0378 - mean_absolute_error: 0.1421 - mean_squared_error: 0.0378 - val_loss: 0.0372 - val_mean_absolute_error: 0.1590 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 6/6\n",
            "24828/24828 [==============================] - 51s 2ms/step - loss: 0.0500 - mean_absolute_error: 0.1767 - mean_squared_error: 0.0500 - val_loss: 0.0358 - val_mean_absolute_error: 0.1512 - val_mean_squared_error: 0.0358\n",
            "model6003, epoch 6\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 88s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0216 - val_loss: 0.0140 - val_mean_absolute_error: 0.0953 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0947 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1021 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0451 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1038 - mean_squared_error: 0.0227 - val_loss: 0.1021 - val_mean_absolute_error: 0.2609 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1769 - mean_squared_error: 0.0499 - val_loss: 0.0276 - val_mean_absolute_error: 0.1351 - val_mean_squared_error: 0.0276\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0163 - mean_absolute_error: 0.0876 - mean_squared_error: 0.0163 - val_loss: 0.0609 - val_mean_absolute_error: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0460 - mean_absolute_error: 0.1522 - mean_squared_error: 0.0460 - val_loss: 0.0627 - val_mean_absolute_error: 0.1907 - val_mean_squared_error: 0.0627\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1740 - mean_squared_error: 0.0482 - val_loss: 0.0317 - val_mean_absolute_error: 0.1462 - val_mean_squared_error: 0.0317\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0214 - mean_absolute_error: 0.1201 - mean_squared_error: 0.0214 - val_loss: 0.0103 - val_mean_absolute_error: 0.0825 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0640 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0965 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0377 - mean_absolute_error: 0.1417 - mean_squared_error: 0.0377 - val_loss: 0.0372 - val_mean_absolute_error: 0.1591 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 6/6\n",
            "24828/24828 [==============================] - 50s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1762 - mean_squared_error: 0.0499 - val_loss: 0.0381 - val_mean_absolute_error: 0.1548 - val_mean_squared_error: 0.0381\n",
            "model6004, epoch 6\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0217 - mean_absolute_error: 0.1183 - mean_squared_error: 0.0217 - val_loss: 0.0140 - val_mean_absolute_error: 0.0952 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0133 - mean_absolute_error: 0.0939 - mean_squared_error: 0.0133 - val_loss: 0.0142 - val_mean_absolute_error: 0.0991 - val_mean_squared_error: 0.0142\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0064 - mean_absolute_error: 0.0638 - mean_squared_error: 0.0064 - val_loss: 0.0030 - val_mean_absolute_error: 0.0457 - val_mean_squared_error: 0.0030\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0218 - mean_absolute_error: 0.1011 - mean_squared_error: 0.0218 - val_loss: 0.1055 - val_mean_absolute_error: 0.2662 - val_mean_squared_error: 0.1055\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0487 - mean_absolute_error: 0.1748 - mean_squared_error: 0.0487 - val_loss: 0.0281 - val_mean_absolute_error: 0.1370 - val_mean_squared_error: 0.0281\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0131 - mean_absolute_error: 0.0800 - mean_squared_error: 0.0131 - val_loss: 0.0885 - val_mean_absolute_error: 0.2306 - val_mean_squared_error: 0.0885\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0446 - mean_absolute_error: 0.1502 - mean_squared_error: 0.0446 - val_loss: 0.0588 - val_mean_absolute_error: 0.1826 - val_mean_squared_error: 0.0588\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0457 - mean_absolute_error: 0.1688 - mean_squared_error: 0.0457 - val_loss: 0.0324 - val_mean_absolute_error: 0.1463 - val_mean_squared_error: 0.0324\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0206 - mean_absolute_error: 0.1170 - mean_squared_error: 0.0206 - val_loss: 0.0107 - val_mean_absolute_error: 0.0845 - val_mean_squared_error: 0.0107\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0637 - mean_squared_error: 0.0066 - val_loss: 0.0112 - val_mean_absolute_error: 0.0924 - val_mean_squared_error: 0.0112\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0356 - mean_absolute_error: 0.1375 - mean_squared_error: 0.0356 - val_loss: 0.0386 - val_mean_absolute_error: 0.1608 - val_mean_squared_error: 0.0386\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 6/6\n",
            "24828/24828 [==============================] - 50s 2ms/step - loss: 0.0471 - mean_absolute_error: 0.1706 - mean_squared_error: 0.0471 - val_loss: 0.0522 - val_mean_absolute_error: 0.1841 - val_mean_squared_error: 0.0522\n",
            "model6005, epoch 6\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0202 - mean_absolute_error: 0.1140 - mean_squared_error: 0.0202 - val_loss: 0.0150 - val_mean_absolute_error: 0.0989 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0120 - mean_absolute_error: 0.0884 - mean_squared_error: 0.0120 - val_loss: 0.0147 - val_mean_absolute_error: 0.1001 - val_mean_squared_error: 0.0147\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0059 - mean_absolute_error: 0.0608 - mean_squared_error: 0.0059 - val_loss: 0.0031 - val_mean_absolute_error: 0.0465 - val_mean_squared_error: 0.0031\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0164 - mean_absolute_error: 0.0891 - mean_squared_error: 0.0164 - val_loss: 0.1130 - val_mean_absolute_error: 0.2721 - val_mean_squared_error: 0.1130\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0385 - mean_absolute_error: 0.1538 - mean_squared_error: 0.0385 - val_loss: 0.0288 - val_mean_absolute_error: 0.1372 - val_mean_squared_error: 0.0288\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0108 - mean_absolute_error: 0.0745 - mean_squared_error: 0.0108 - val_loss: 0.0857 - val_mean_absolute_error: 0.2279 - val_mean_squared_error: 0.0857\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0347 - mean_absolute_error: 0.1348 - mean_squared_error: 0.0347 - val_loss: 0.0618 - val_mean_absolute_error: 0.1897 - val_mean_squared_error: 0.0618\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0347 - mean_absolute_error: 0.1449 - mean_squared_error: 0.0347 - val_loss: 0.0371 - val_mean_absolute_error: 0.1564 - val_mean_squared_error: 0.0371\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0177 - mean_absolute_error: 0.1064 - mean_squared_error: 0.0177 - val_loss: 0.0124 - val_mean_absolute_error: 0.0908 - val_mean_squared_error: 0.0124\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0059 - mean_absolute_error: 0.0600 - mean_squared_error: 0.0059 - val_loss: 0.0097 - val_mean_absolute_error: 0.0838 - val_mean_squared_error: 0.0097\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0252 - mean_absolute_error: 0.1157 - mean_squared_error: 0.0252 - val_loss: 0.0411 - val_mean_absolute_error: 0.1657 - val_mean_squared_error: 0.0411\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 6/6\n",
            "24828/24828 [==============================] - 50s 2ms/step - loss: 0.0349 - mean_absolute_error: 0.1449 - mean_squared_error: 0.0349 - val_loss: 0.0461 - val_mean_absolute_error: 0.1683 - val_mean_squared_error: 0.0461\n",
            "model6006, epoch 6\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0216 - mean_absolute_error: 0.1182 - mean_squared_error: 0.0216 - val_loss: 0.0140 - val_mean_absolute_error: 0.0953 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0135 - mean_absolute_error: 0.0948 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1020 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0451 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0227 - mean_absolute_error: 0.1039 - mean_squared_error: 0.0227 - val_loss: 0.1021 - val_mean_absolute_error: 0.2609 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1770 - mean_squared_error: 0.0499 - val_loss: 0.0278 - val_mean_absolute_error: 0.1350 - val_mean_squared_error: 0.0278\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0164 - mean_absolute_error: 0.0878 - mean_squared_error: 0.0164 - val_loss: 0.0609 - val_mean_absolute_error: 0.1901 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 86s 2ms/step - loss: 0.0460 - mean_absolute_error: 0.1522 - mean_squared_error: 0.0460 - val_loss: 0.0629 - val_mean_absolute_error: 0.1913 - val_mean_squared_error: 0.0629\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1740 - mean_squared_error: 0.0482 - val_loss: 0.0318 - val_mean_absolute_error: 0.1465 - val_mean_squared_error: 0.0318\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0214 - mean_absolute_error: 0.1202 - mean_squared_error: 0.0214 - val_loss: 0.0103 - val_mean_absolute_error: 0.0824 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0066 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0963 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 6/6\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0377 - mean_absolute_error: 0.1418 - mean_squared_error: 0.0377 - val_loss: 0.0372 - val_mean_absolute_error: 0.1590 - val_mean_squared_error: 0.0372\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 6/6\n",
            "24828/24828 [==============================] - 50s 2ms/step - loss: 0.0500 - mean_absolute_error: 0.1767 - mean_squared_error: 0.0500 - val_loss: 0.0353 - val_mean_absolute_error: 0.1514 - val_mean_squared_error: 0.0353\n",
            "model6001, epoch 7\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0239 - mean_absolute_error: 0.1242 - mean_squared_error: 0.0239 - val_loss: 0.0140 - val_mean_absolute_error: 0.0954 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0157 - mean_absolute_error: 0.1015 - mean_squared_error: 0.0157 - val_loss: 0.0149 - val_mean_absolute_error: 0.1017 - val_mean_squared_error: 0.0149\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0093 - mean_absolute_error: 0.0769 - mean_squared_error: 0.0093 - val_loss: 0.0029 - val_mean_absolute_error: 0.0453 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0252 - mean_absolute_error: 0.1119 - mean_squared_error: 0.0252 - val_loss: 0.1022 - val_mean_absolute_error: 0.2607 - val_mean_squared_error: 0.1022\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0524 - mean_absolute_error: 0.1813 - mean_squared_error: 0.0524 - val_loss: 0.0277 - val_mean_absolute_error: 0.1351 - val_mean_squared_error: 0.0277\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0188 - mean_absolute_error: 0.0986 - mean_squared_error: 0.0188 - val_loss: 0.0609 - val_mean_absolute_error: 0.1905 - val_mean_squared_error: 0.0609\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1586 - mean_squared_error: 0.0482 - val_loss: 0.0627 - val_mean_absolute_error: 0.1914 - val_mean_squared_error: 0.0627\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0509 - mean_absolute_error: 0.1789 - mean_squared_error: 0.0509 - val_loss: 0.0318 - val_mean_absolute_error: 0.1465 - val_mean_squared_error: 0.0318\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0238 - mean_absolute_error: 0.1259 - mean_squared_error: 0.0238 - val_loss: 0.0103 - val_mean_absolute_error: 0.0825 - val_mean_squared_error: 0.0103\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0754 - mean_squared_error: 0.0090 - val_loss: 0.0119 - val_mean_absolute_error: 0.0971 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0401 - mean_absolute_error: 0.1480 - mean_squared_error: 0.0401 - val_loss: 0.0373 - val_mean_absolute_error: 0.1592 - val_mean_squared_error: 0.0373\n",
            "Train on 24828 samples, validate on 4382 samples\n",
            "Epoch 7/7\n",
            "24828/24828 [==============================] - 50s 2ms/step - loss: 0.0525 - mean_absolute_error: 0.1808 - mean_squared_error: 0.0525 - val_loss: 0.0362 - val_mean_absolute_error: 0.1537 - val_mean_squared_error: 0.0362\n",
            "model6002, epoch 7\n",
            "image 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 87s 2ms/step - loss: 0.0218 - mean_absolute_error: 0.1186 - mean_squared_error: 0.0218 - val_loss: 0.0140 - val_mean_absolute_error: 0.0952 - val_mean_squared_error: 0.0140\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0951 - mean_squared_error: 0.0136 - val_loss: 0.0150 - val_mean_absolute_error: 0.1021 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0068 - mean_absolute_error: 0.0658 - mean_squared_error: 0.0068 - val_loss: 0.0029 - val_mean_absolute_error: 0.0452 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0228 - mean_absolute_error: 0.1041 - mean_squared_error: 0.0228 - val_loss: 0.1021 - val_mean_absolute_error: 0.2608 - val_mean_squared_error: 0.1021\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "42500/42500 [==============================] - 85s 2ms/step - loss: 0.0500 - mean_absolute_error: 0.1771 - mean_squared_error: 0.0500 - val_loss: 0.0276 - val_mean_absolute_error: 0.1351 - val_mean_squared_error: 0.0276\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 7/7\n",
            "20736/42500 [=============>................] - ETA: 41s - loss: 0.0162 - mean_absolute_error: 0.0876 - mean_squared_error: 0.0162"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f45496ff323a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0;31m#fitting model over one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m#saving model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f45496ff323a>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, images, time_steps, predictions, batch_size, epoch)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 verbose=1)\n\u001b[0m\u001b[1;32m    183\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69roXlQrRFmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "265f262d-483e-4ce1-cc95-b5e06080c8bc"
      },
      "source": [
        "#Using model and real disc images\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import PIL.Image as Image\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "  \n",
        "def test_model(test_images, model, time_steps, predictions, window_width):\n",
        "  testing = test_images\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "\n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "data_dir = \"/content/drive/My Drive/disc_data\"\n",
        "\n",
        "model_version = 5004\n",
        "model_epoch = 1\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "time_steps = 15\n",
        "predictions = 1\n",
        "window_width = 360\n",
        "  \n",
        "model_name = 'model'+str(model_version)\n",
        "res_dir = os.path.join(results_dir, model_name)\n",
        "if not os.path.isdir(res_dir):\n",
        "  os.makedirs(res_dir)\n",
        "\n",
        "for model_epoch in range(13, 14):\n",
        "  print('epoch : '+str(model_epoch))\n",
        "  model_weights_name = model_name+'_epoch'+str(model_epoch)+'.h5'\n",
        "  model = load_model(os.path.join(models_dir, model_name, model_weights_name))\n",
        "  \n",
        "  images = os.listdir(data_dir)\n",
        "  for image in images:\n",
        "    if image.endswith('.tif'):\n",
        "      groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "      groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "      groove = normalize_u8bits(groove)\n",
        "\n",
        "      track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "      root = track_data.getroot()\n",
        "      track = []\n",
        "      for x in root.findall('ArrayOfDouble'):\n",
        "        point = []\n",
        "        for d in x.findall('double'):\n",
        "          point.append(int(d.text))\n",
        "        track.append(point)  \n",
        "      track.pop(0)\n",
        "\n",
        "      total_pred = np.empty((len(track), 2))\n",
        "\n",
        "      for i in range(0, len(track), 50000):\n",
        "        print('starting block : '+str(i))\n",
        "        x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "        window=x_data[0]*255 \n",
        "        #print(window.shape)\n",
        "        window=np.squeeze(window)\n",
        "        window = np.array(window)\n",
        "        window = window.astype('uint8')\n",
        "        im = Image.fromarray(window)\n",
        "        im.save(os.path.join(res_dir, image.replace('.tif','_point_'+str(track[i])+'.tif')))\n",
        "        prediction = model.predict(x_data)\n",
        "        prediction = prediction.astype(float)\n",
        "        pred_audio = np.column_stack((prediction, prediction))\n",
        "        for index, line in enumerate(pred_audio):\n",
        "          total_pred[i+index] = line\n",
        "        #print(pred_audio.shape)\n",
        "        #total_pred = np.append(total_pred, pred_audio)\n",
        "        #print(total_pred.shape)\n",
        "      \n",
        "      scipy.io.wavfile.write(\n",
        "          os.path.join(\n",
        "              res_dir, \n",
        "              image+'_model'+str(model_version)+'_epoch'+str(model_epoch)+'_pred.wav'), 104000, total_pred)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0808 10:35:51.399158 140294032181120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0808 10:35:51.441619 140294032181120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0808 10:35:51.493452 140294032181120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0808 10:35:51.500994 140294032181120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0808 10:35:51.502275 140294032181120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0808 10:35:51.512764 140294032181120 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch : 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0808 10:35:52.061638 140294032181120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0808 10:35:55.494295 140294032181120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting block : 0\n",
            "starting block : 50000\n",
            "starting block : 100000\n",
            "starting block : 150000\n",
            "starting block : 200000\n",
            "starting block : 250000\n",
            "starting block : 300000\n",
            "starting block : 350000\n",
            "starting block : 400000\n",
            "starting block : 450000\n",
            "starting block : 500000\n",
            "starting block : 550000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoxBEB1HA5tL",
        "colab_type": "code",
        "outputId": "edb9b18c-7c8c-41b6-df21-28c44b172633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#transfer learning from model 5000\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#normalize signed 32-bits numpy array to [-1;1]\n",
        "def normalize_32bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+2147483648)*(2))/4294967295)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "    \n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "  \n",
        "def test_model(test_images, model, time_steps, predictions, window_width):\n",
        "  testing = test_images\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "\n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "data_dir = \"/content/drive/My Drive/disc_data\"\n",
        "\n",
        "model_version = 5000\n",
        "model_epoch = 5\n",
        "\n",
        "new_model_version = 5001\n",
        "new_model_epoch_start = 1\n",
        "new_model_epoch_quantity = 5\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "time_steps = 15\n",
        "predictions = 1\n",
        "window_width = 320\n",
        "\n",
        "\n",
        "\n",
        "if new_model_epoch_start == 1:\n",
        "  model_name = 'model'+str(model_version)\n",
        "  model_weights_name = model_name+'_epoch'+str(model_epoch)+'.h5'\n",
        "else:\n",
        "  model_name = 'model'+str(new_model_version)\n",
        "  model_weights_name = model_name+'_epoch'+str(new_model_epoch_start-1)+'.h5'\n",
        "  \n",
        "model_dir = os.path.join(models_dir, model_name)\n",
        "model = load_model(os.path.join(model_dir, model_weights_name))\n",
        "\n",
        "save_model_dir = os.path.join(models_dir, 'model'+str(new_model_version))\n",
        "if not os.path.isdir(save_model_dir):\n",
        "  os.makedirs(save_model_dir)\n",
        "\n",
        "  \n",
        "images = os.listdir(data_dir)\n",
        "for new_model_epoch in range(new_model_epoch_start, new_model_epoch_start+new_model_epoch_quantity):\n",
        "  for image in images:\n",
        "    if not image.endswith('.tif'):\n",
        "      continue\n",
        "    #read image\n",
        "    groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "    groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "    groove = normalize_u8bits(groove)\n",
        "\n",
        "    #read track\n",
        "    track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "    root = track_data.getroot()\n",
        "    track = []\n",
        "    for x in root.findall('ArrayOfDouble'):\n",
        "      point = []\n",
        "      for d in x.findall('double'):\n",
        "        point.append(int(d.text))\n",
        "      track.append(point)  \n",
        "    track.pop(0)\n",
        "\n",
        "    #read sound\n",
        "    samplerate, sound=read(os.path.join(data_dir, image.replace('.tif', '.wav')))\n",
        "    sound = np.array(sound[0:len(track)])\n",
        "    sound = normalize_16bits(sound)\n",
        "\n",
        "    for i in range(0, len(track), 50000):\n",
        "      x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "      y_data = sound[i:min(len(track), i+50000)]\n",
        "      model.fit(x_data, y_data, \n",
        "                batch_size=32, \n",
        "                epochs=new_model_epoch+1, \n",
        "                initial_epoch=new_model_epoch, \n",
        "                shuffle=True,\n",
        "                validation_split=0.15,\n",
        "                verbose=1)\n",
        "  model.save(os.path.join(save_model_dir, 'model'+str(new_model_version)+'_epoch'+str(new_model_epoch)+'.h5'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 55s 1ms/step - loss: 0.0247 - mean_absolute_error: 0.1218 - mean_squared_error: 0.0247 - val_loss: 0.0142 - val_mean_absolute_error: 0.0959 - val_mean_squared_error: 0.0142\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0135 - mean_absolute_error: 0.0948 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1023 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0452 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0227 - mean_absolute_error: 0.1038 - mean_squared_error: 0.0227 - val_loss: 0.1024 - val_mean_absolute_error: 0.2621 - val_mean_squared_error: 0.1024\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0499 - mean_absolute_error: 0.1771 - mean_squared_error: 0.0499 - val_loss: 0.0277 - val_mean_absolute_error: 0.1360 - val_mean_squared_error: 0.0277\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0162 - mean_absolute_error: 0.0875 - mean_squared_error: 0.0162 - val_loss: 0.0691 - val_mean_absolute_error: 0.2010 - val_mean_squared_error: 0.0691\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0459 - mean_absolute_error: 0.1524 - mean_squared_error: 0.0459 - val_loss: 0.0634 - val_mean_absolute_error: 0.1902 - val_mean_squared_error: 0.0634\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 53s 1ms/step - loss: 0.0477 - mean_absolute_error: 0.1731 - mean_squared_error: 0.0477 - val_loss: 0.0320 - val_mean_absolute_error: 0.1467 - val_mean_squared_error: 0.0320\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0214 - mean_absolute_error: 0.1199 - mean_squared_error: 0.0214 - val_loss: 0.0105 - val_mean_absolute_error: 0.0833 - val_mean_squared_error: 0.0105\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0066 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0963 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0373 - mean_absolute_error: 0.1409 - mean_squared_error: 0.0373 - val_loss: 0.0375 - val_mean_absolute_error: 0.1593 - val_mean_squared_error: 0.0375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-41e3aedb54e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 verbose=1)\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_epoch'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
            "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 29210 input samples and 27287 target samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuwhX5qmsPeS",
        "colab_type": "code",
        "outputId": "98ef8da5-7e6d-4ab9-9ba6-a3700b2cbcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}