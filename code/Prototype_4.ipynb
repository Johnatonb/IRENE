{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prototype_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fniv64S3PwW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training model 5000 (pure sine, same dimensions as disc groove)\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one np array as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions):\n",
        "  reshaped = []\n",
        "  for i in range(0, data.shape[0]+1-time_steps, predictions):\n",
        "    reshaped.append(data[i:i+time_steps,:])\n",
        "  return np.array(reshaped)\n",
        "\n",
        "#generate a perfect sine wave with amplitude range [-1;1] in a format ready for goal\n",
        "def generate_sine(freq, size, offset, predictions):\n",
        "  timestep = 1.0/104000.0\n",
        "  sine = []\n",
        "  block = []\n",
        "  for i in range(size):\n",
        "    amp = math.cos(2*math.pi*freq*(i+offset)*timestep)\n",
        "    block.append(amp)\n",
        "    if (i+1) % predictions == 0:\n",
        "      sine.append(block)\n",
        "      block = []\n",
        "  return np.array(sine)\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "\n",
        "#create model\n",
        "def create_model(filter_shapes, \n",
        "                 time_steps, \n",
        "                 img_width, \n",
        "                 activation_function, \n",
        "                 max_pool_shape, \n",
        "                 dropout_rate, \n",
        "                 predictions, \n",
        "                 padding,\n",
        "                 learning_rate,\n",
        "                 decay,\n",
        "                 model_dir,\n",
        "                 model_name):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, filter_shapes[0],\n",
        "                   input_shape=(time_steps, img_width, 1), padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, filter_shapes[1], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))   \n",
        "  if not dropout_rate[0]==0.0:\n",
        "    model.add(Dropout(dropout_rate[0]))\n",
        "  \n",
        "  model.add(Conv2D(64, filter_shapes[2], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, filter_shapes[3], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))\n",
        "  if not dropout_rate[1]==0.0:\n",
        "    model.add(Dropout(dropout_rate[1]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not dropout_rate[2]==0.0:\n",
        "    model.add(Dropout(dropout_rate[2]))\n",
        "  model.add(Dense(predictions))\n",
        "  model.add(Activation('tanh'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=opt,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "            \n",
        "  with open(os.path.join(model_dir, model_name+'_parameters.txt'), 'w') as f:\n",
        "    f.write('time_steps_a : '+str(time_steps)+'\\n')\n",
        "    f.write('predictions : ' + str(predictions)+'\\n')\n",
        "    f.write('filter_shapes : '+str(filter_shapes)+'\\n')\n",
        "    f.write('max_pool_shape : '+str(max_pool_shape)+'\\n')\n",
        "    f.write('dropout_rate : '+str(dropout_rate)+'\\n')\n",
        "    f.write('activation : '+activation_function+'\\n')\n",
        "    f.write('padding : '+padding+'\\n')\n",
        "    f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "    f.write('decay : '+str(decay)+'\\n')\n",
        "    f.write('optimizer : '+str(opt)+'\\n')\n",
        "  \n",
        "  model_first_save(model, model_dir, model_name)\n",
        "  return model\n",
        "  \n",
        "def model_first_save(model, model_dir, model_name):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  plot_model(model, to_file=os.path.join(model_dir, model_name+'_architecture.png'), show_shapes=True, show_layer_names=False)\n",
        "  with open(os.path.join(model_dir, model_name+'_summary.txt'), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "def fit_model(model, images, time_steps, predictions, batch_size, epoch):\n",
        "  counter=1\n",
        "  for image_name in images:\n",
        "    print('image '+str(counter)+'/'+str(len(images)))\n",
        "    #print('name : '+image_name)\n",
        "    counter+=1\n",
        "    img = io.imread(os.path.join(img_train_dir, image_name) , as_gray=True)\n",
        "    img = noising(img)\n",
        "    img = img.reshape(img.shape[0], img.shape[1], 1)\n",
        "    img_norm = normalize_u8bits(img)\n",
        "    x_train = window_reshape(img_norm, time_steps, predictions)\n",
        "\n",
        "    freq = int(image_name[1:image_name.find('_')])\n",
        "\n",
        "    y_train = generate_sine(freq, x_train.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epoch,\n",
        "                        initial_epoch=epoch-1,\n",
        "                        shuffle=True,\n",
        "                        validation_split=0.15,\n",
        "                        verbose=1)\n",
        "  return history\n",
        "  \n",
        "def test_model(model, time_steps, predictions):\n",
        "  testing = os.listdir(img_test_dir)\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    #img_test = noising(img_test)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "\n",
        "def noising(img, seed=None):\n",
        "    \n",
        "    random.seed(seed)\n",
        "    #scratches generator\n",
        "    for i in range(40000):\n",
        "        h = random.randint(1, 3)\n",
        "        w = random.randint(50, 150)\n",
        "        y = random.randint(0, img.shape[0]+1)\n",
        "        x = random.randint(0, img.shape[1]+1)\n",
        "\n",
        "        img[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "        \n",
        "    #holes generator\n",
        "    for i in range(8000):\n",
        "        h = random.randint(5, 15)\n",
        "        w = random.randint(10, 100)\n",
        "        y = random.randint(0, img.shape[0]+1)\n",
        "        x = random.randint(0, img.shape[1]+1)\n",
        "\n",
        "        img[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "        \n",
        "    np.random.seed(seed)\n",
        "    gauss = np.random.normal(-100.0,50.0,img.shape)\n",
        "    noisy = img + gauss\n",
        "    clipped = np.clip(noisy, 0, 255)\n",
        "    \n",
        "    return np.around(clipped).astype(np.uint8)\n",
        "\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "img_train_dir = \"/content/drive/My Drive/disc_sim_data/train\"\n",
        "img_test_dir = \"/content/drive/My Drive/disc_sim_data/noisy_test\"\n",
        "\n",
        "model_version_start = 5000 #for output numbering\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "epoch_start = 2\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "time_steps = 15\n",
        "predictions = 1\n",
        "filter_shapes = [(3,3),(3,3),(3,3),(3,3)]\n",
        "img_width = 320\n",
        "learning_rate = 0.001\n",
        "decay = 0.0\n",
        "steps_for_testing = 100 # 1 step = 1 image\n",
        "max_pool_shape = (2, 2)\n",
        "dropout_rate = [0.25, 0.25, 0.5]\n",
        "activation_function = 'relu'\n",
        "padding='same'\n",
        "division = 5 # epoch division for testing and saving\n",
        "\n",
        "current_epoch = 1\n",
        "current_mv = 0\n",
        "\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  \n",
        "  model_version = model_version_start\n",
        "  \n",
        "  images = os.listdir(img_train_dir)\n",
        "  shuffle(images)\n",
        "  \n",
        "  model_name = 'model'+str(model_version)\n",
        "\n",
        "  print(model_name+', epoch '+str(epoch))\n",
        "  \n",
        "  model_dir = os.path.join(models_dir, model_name)\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  #loading model\n",
        "  if epoch > 1:\n",
        "    model = load_model(os.path.join(model_dir, model_name+'_epoch'+str(epoch-1)+'.h5'))\n",
        "  else:\n",
        "\n",
        "    model = create_model(filter_shapes=filter_shapes,\n",
        "                        time_steps=time_steps,\n",
        "                        img_width=img_width,\n",
        "                        activation_function=activation_function,\n",
        "                        max_pool_shape=max_pool_shape,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        predictions=predictions,\n",
        "                        padding=padding,\n",
        "                        learning_rate = learning_rate,\n",
        "                        decay = decay,\n",
        "                        model_dir = model_dir,\n",
        "                        model_name = model_name)\n",
        "  size = len(images)//division\n",
        "  for nth in range(0, division) :\n",
        "    #fitting model over one epoch\n",
        "    train_results = fit_model(model, images[nth*size:(nth+1)*size], time_steps, predictions, batch_size, epoch)\n",
        "\n",
        "    #saving model\n",
        "    stamp = str(epoch-1)+'.'+str(nth+1) if nth < division-1 else str(epoch)\n",
        "    model.save(os.path.join(model_dir, model_name+'_epoch'+stamp+'.h5'))\n",
        "\n",
        "    #testing model\n",
        "    test_results = test_model(model, time_steps, predictions)\n",
        "\n",
        "    #appending results\n",
        "    result_dir = os.path.join(results_dir, model_name)\n",
        "    if epoch==1 and nth == 0:\n",
        "      results = {}\n",
        "      results['mae_validation']={}\n",
        "      results['mse_validation']={}\n",
        "      results['mae_test']={}\n",
        "      results['mse_test']={}\n",
        "      if not os.path.isdir(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "    else:\n",
        "      with open(os.path.join(result_dir, model_name+'_results.json'), 'r') as fp:\n",
        "        results = json.load(fp)\n",
        "\n",
        "    results['mae_validation'][stamp] = train_results.history['val_mean_absolute_error']\n",
        "    results['mse_validation'][stamp] = train_results.history['val_mean_squared_error']\n",
        "    results['mae_test'][stamp] = test_results[1]\n",
        "    results['mse_test'][stamp] = test_results[0]\n",
        "\n",
        "    with open(os.path.join(result_dir, model_name+'_results.json'), 'w') as f:\n",
        "      json.dump(results, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-S0UiAIO0kA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training model 5002 (real disc groove, sound from tape)\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "    \n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "\n",
        "#create model\n",
        "def create_model(filter_shapes, \n",
        "                 time_steps, \n",
        "                 img_width, \n",
        "                 activation_function, \n",
        "                 max_pool_shape, \n",
        "                 dropout_rate, \n",
        "                 predictions, \n",
        "                 padding,\n",
        "                 learning_rate,\n",
        "                 decay,\n",
        "                 model_dir,\n",
        "                 model_name):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, filter_shapes[0],\n",
        "                   input_shape=(time_steps, img_width, 1), padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, filter_shapes[1], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))   \n",
        "  if not dropout_rate[0]==0.0:\n",
        "    model.add(Dropout(dropout_rate[0]))\n",
        "  \n",
        "  model.add(Conv2D(64, filter_shapes[2], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, filter_shapes[3], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))\n",
        "  if not dropout_rate[1]==0.0:\n",
        "    model.add(Dropout(dropout_rate[1]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not dropout_rate[2]==0.0:\n",
        "    model.add(Dropout(dropout_rate[2]))\n",
        "  model.add(Dense(predictions))\n",
        "  model.add(Activation('tanh'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=opt,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "            \n",
        "  with open(os.path.join(model_dir, model_name+'_parameters.txt'), 'w') as f:\n",
        "    f.write('time_steps_a : '+str(time_steps)+'\\n')\n",
        "    f.write('predictions : ' + str(predictions)+'\\n')\n",
        "    f.write('filter_shapes : '+str(filter_shapes)+'\\n')\n",
        "    f.write('max_pool_shape : '+str(max_pool_shape)+'\\n')\n",
        "    f.write('dropout_rate : '+str(dropout_rate)+'\\n')\n",
        "    f.write('activation : '+activation_function+'\\n')\n",
        "    f.write('padding : '+padding+'\\n')\n",
        "    f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "    f.write('decay : '+str(decay)+'\\n')\n",
        "    f.write('optimizer : '+str(opt)+'\\n')\n",
        "  \n",
        "  model_first_save(model, model_dir, model_name)\n",
        "  return model\n",
        "  \n",
        "def model_first_save(model, model_dir, model_name):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  plot_model(model, to_file=os.path.join(model_dir, model_name+'_architecture.png'), show_shapes=True, show_layer_names=False)\n",
        "  with open(os.path.join(model_dir, model_name+'_summary.txt'), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "def fit_model(model, images, time_steps, predictions, batch_size, epoch):\n",
        "  counter=1  \n",
        "  for image in images:\n",
        "    print('image '+str(counter)+'/'+str(len(images)))\n",
        "    #print('name : '+image_name)\n",
        "    counter+=1\n",
        "    if not image.endswith('.tif'):\n",
        "      continue\n",
        "    #read image\n",
        "    groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "    groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "    groove = normalize_u8bits(groove)\n",
        "\n",
        "    #read track\n",
        "    track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "    root = track_data.getroot()\n",
        "    track = []\n",
        "    for x in root.findall('ArrayOfDouble'):\n",
        "      point = []\n",
        "      for d in x.findall('double'):\n",
        "        point.append(int(d.text))\n",
        "      track.append(point)  \n",
        "    track.pop(0)\n",
        "\n",
        "    #read sound\n",
        "    samplerate, sound=read(os.path.join(data_dir, image.replace('.tif', '.wav')))\n",
        "    sound = np.array(sound[0:len(track)])\n",
        "    sound = normalize_16bits(sound)\n",
        "\n",
        "    for i in range(0, len(track), 50000):\n",
        "      x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "      y_data = sound[i:min(len(track), i+50000)]\n",
        "      history = model.fit(x_data, y_data, \n",
        "                batch_size=32, \n",
        "                epochs=epoch,\n",
        "                initial_epoch=epoch-1,\n",
        "                shuffle=True,\n",
        "                validation_split=0.15,\n",
        "                verbose=1)\n",
        "  return history\n",
        "  \n",
        "def test_model(model, time_steps, predictions):\n",
        "  testing = os.listdir(img_test_dir)\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    #img_test = noising(img_test)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "data_dir = \"/content/drive/My Drive/disc_data\"\n",
        "\n",
        "\n",
        "model_version_start = 5002 #for output numbering\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "epoch_start = 1\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "time_steps = 21\n",
        "predictions = 1\n",
        "filter_shapes = [(3,3),(3,3),(3,3),(3,3)]\n",
        "img_width = 360\n",
        "window_width = 360\n",
        "learning_rate = 0.0005\n",
        "decay = 0\n",
        "steps_for_testing = 100 # 1 step = 1 image\n",
        "max_pool_shape = (2, 2)\n",
        "dropout_rate = [0.5, 0.5, 0.7]\n",
        "activation_function = 'relu'\n",
        "padding='same'\n",
        "division = 1 # epoch division for testing and saving\n",
        "\n",
        "current_epoch = 1\n",
        "current_mv = 0\n",
        "\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  \n",
        "  model_version = model_version_start\n",
        "  \n",
        "  images_raw = os.listdir(data_dir)\n",
        "  images=[]\n",
        "  for image in images_raw:\n",
        "    if image.endswith('.tif'):\n",
        "      images.append(image)\n",
        "  \n",
        "  shuffle(images)\n",
        "  model_name = 'model'+str(model_version)\n",
        "\n",
        "  print(model_name+', epoch '+str(epoch))\n",
        "  \n",
        "  model_dir = os.path.join(models_dir, model_name)\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  #loading model\n",
        "  if epoch > 1:\n",
        "    model = load_model(os.path.join(model_dir, model_name+'_epoch'+str(epoch-1)+'.h5'))\n",
        "  else:\n",
        "    model = create_model(filter_shapes=filter_shapes,\n",
        "                        time_steps=time_steps,\n",
        "                        img_width=img_width,\n",
        "                        activation_function=activation_function,\n",
        "                        max_pool_shape=max_pool_shape,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        predictions=predictions,\n",
        "                        padding=padding,\n",
        "                        learning_rate = learning_rate,\n",
        "                        decay = decay,\n",
        "                        model_dir = model_dir,\n",
        "                        model_name = model_name)\n",
        "  size = len(images)//division\n",
        "  for nth in range(0, division) :\n",
        "    #fitting model over one epoch\n",
        "    train_results = fit_model(model, images[nth*size:(nth+1)*size], time_steps, predictions, batch_size, epoch)\n",
        "\n",
        "    #saving model\n",
        "    stamp = str(epoch-1)+'.'+str(nth+1) if nth < division-1 else str(epoch)\n",
        "    model.save(os.path.join(model_dir, model_name+'_epoch'+stamp+'.h5'))\n",
        "\n",
        "    #testing model\n",
        "    #test_results = test_model(model, time_steps, predictions)\n",
        "\n",
        "    #appending results\n",
        "    result_dir = os.path.join(results_dir, model_name)\n",
        "    if epoch==1 and nth == 0:\n",
        "      results = {}\n",
        "      results['mae_validation']={}\n",
        "      results['mse_validation']={}\n",
        "      #results['mae_test']={}\n",
        "      #results['mse_test']={}\n",
        "      if not os.path.isdir(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "    else:\n",
        "      with open(os.path.join(result_dir, model_name+'_results.json'), 'r') as fp:\n",
        "        results = json.load(fp)\n",
        "\n",
        "    results['mae_validation'][stamp] = train_results.history['val_mean_absolute_error']\n",
        "    results['mse_validation'][stamp] = train_results.history['val_mean_squared_error']\n",
        "    #results['mae_test'][stamp] = test_results[1]\n",
        "    #results['mse_test'][stamp] = test_results[0]\n",
        "\n",
        "    with open(os.path.join(result_dir, model_name+'_results.json'), 'w') as f:\n",
        "      json.dump(results, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjbU6-M6mgr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finding best learning rate for Adam\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "    \n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "\n",
        "#create model\n",
        "def create_model(filter_shapes, \n",
        "                 time_steps, \n",
        "                 img_width, \n",
        "                 activation_function, \n",
        "                 max_pool_shape, \n",
        "                 dropout_rate, \n",
        "                 predictions, \n",
        "                 padding,\n",
        "                 learning_rate,\n",
        "                 decay,\n",
        "                 model_dir,\n",
        "                 model_name):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, filter_shapes[0],\n",
        "                   input_shape=(time_steps, img_width, 1), padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, filter_shapes[1], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))   \n",
        "  if not dropout_rate[0]==0.0:\n",
        "    model.add(Dropout(dropout_rate[0]))\n",
        "  \n",
        "  model.add(Conv2D(64, filter_shapes[2], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, filter_shapes[3], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))\n",
        "  if not dropout_rate[1]==0.0:\n",
        "    model.add(Dropout(dropout_rate[1]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not dropout_rate[2]==0.0:\n",
        "    model.add(Dropout(dropout_rate[2]))\n",
        "  model.add(Dense(predictions))\n",
        "  model.add(Activation('tanh'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=opt,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "            \n",
        "  with open(os.path.join(model_dir, model_name+'_parameters.txt'), 'w') as f:\n",
        "    f.write('time_steps_a : '+str(time_steps)+'\\n')\n",
        "    f.write('predictions : ' + str(predictions)+'\\n')\n",
        "    f.write('filter_shapes : '+str(filter_shapes)+'\\n')\n",
        "    f.write('max_pool_shape : '+str(max_pool_shape)+'\\n')\n",
        "    f.write('dropout_rate : '+str(dropout_rate)+'\\n')\n",
        "    f.write('activation : '+activation_function+'\\n')\n",
        "    f.write('padding : '+padding+'\\n')\n",
        "    f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "    f.write('decay : '+str(decay)+'\\n')\n",
        "    f.write('optimizer : '+str(opt)+'\\n')\n",
        "  \n",
        "  model_first_save(model, model_dir, model_name)\n",
        "  return model\n",
        "  \n",
        "def model_first_save(model, model_dir, model_name):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  plot_model(model, to_file=os.path.join(model_dir, model_name+'_architecture.png'), show_shapes=True, show_layer_names=False)\n",
        "  with open(os.path.join(model_dir, model_name+'_summary.txt'), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "def fit_model(model, images, time_steps, predictions, batch_size, epoch):\n",
        "  counter=1  \n",
        "  for image in images:\n",
        "    print('image '+str(counter)+'/'+str(len(images)))\n",
        "    #print('name : '+image_name)\n",
        "    counter+=1\n",
        "    if not image.endswith('.tif'):\n",
        "      continue\n",
        "    #read image\n",
        "    groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "    groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "    groove = normalize_u8bits(groove)\n",
        "\n",
        "    #read track\n",
        "    track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "    root = track_data.getroot()\n",
        "    track = []\n",
        "    for x in root.findall('ArrayOfDouble'):\n",
        "      point = []\n",
        "      for d in x.findall('double'):\n",
        "        point.append(int(d.text))\n",
        "      track.append(point)  \n",
        "    track.pop(0)\n",
        "\n",
        "    #read sound\n",
        "    samplerate, sound=read(os.path.join(data_dir, image.replace('.tif', '.wav')))\n",
        "    sound = np.array(sound[0:len(track)])\n",
        "    sound = normalize_16bits(sound)\n",
        "\n",
        "    for i in range(0, len(track), 50000):\n",
        "      x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "      y_data = sound[i:min(len(track), i+50000)]\n",
        "      history = model.fit(x_data, y_data, \n",
        "                batch_size=32, \n",
        "                epochs=epoch,\n",
        "                initial_epoch=epoch-1,\n",
        "                shuffle=True,\n",
        "                validation_split=0.15,\n",
        "                verbose=1)\n",
        "  return history\n",
        "  \n",
        "def test_model(model, time_steps, predictions):\n",
        "  testing = os.listdir(img_test_dir)\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    #img_test = noising(img_test)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "data_dir = \"/content/drive/My Drive/disc_data\"\n",
        "\n",
        "\n",
        "model_version_start = 6001 #for output numbering\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "epoch_start = 4\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "time_steps = 21\n",
        "predictions = 1\n",
        "filter_shapes = [(3,3),(3,3),(3,3),(3,3)]\n",
        "img_width = 360\n",
        "window_width = 360\n",
        "learning_rate = 0.0005\n",
        "decay = 0\n",
        "steps_for_testing = 100 # 1 step = 1 image\n",
        "max_pool_shape = (2, 2)\n",
        "dropout_rate = [0.25, 0.5, 0.5]\n",
        "activation_function = 'relu'\n",
        "padding='same'\n",
        "division = 1 # epoch division for testing and saving\n",
        "\n",
        "current_epoch = 1\n",
        "current_mv = 0\n",
        "\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  \n",
        "  model_version = model_version_start\n",
        "  \n",
        "  images_raw = os.listdir(data_dir)\n",
        "  images=[]\n",
        "  for image in images_raw:\n",
        "    if image.endswith('.tif'):\n",
        "      images.append(image)\n",
        "  \n",
        "  shuffle(images)\n",
        "  \n",
        "  for learning_rate in [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]:\n",
        "    \n",
        "    model_name = 'model'+str(model_version)\n",
        "\n",
        "    print(model_name+', epoch '+str(epoch))\n",
        "\n",
        "    model_dir = os.path.join(models_dir, model_name)\n",
        "    if not os.path.isdir(model_dir):\n",
        "      os.makedirs(model_dir)\n",
        "\n",
        "    #loading model\n",
        "    if epoch > 1:\n",
        "      model = load_model(os.path.join(model_dir, model_name+'_epoch'+str(epoch-1)+'.h5'))\n",
        "    else:\n",
        "      model = create_model(filter_shapes=filter_shapes,\n",
        "                          time_steps=time_steps,\n",
        "                          img_width=img_width,\n",
        "                          activation_function=activation_function,\n",
        "                          max_pool_shape=max_pool_shape,\n",
        "                          dropout_rate=dropout_rate,\n",
        "                          predictions=predictions,\n",
        "                          padding=padding,\n",
        "                          learning_rate = learning_rate,\n",
        "                          decay = decay,\n",
        "                          model_dir = model_dir,\n",
        "                          model_name = model_name)\n",
        "    size = len(images)//division\n",
        "    for nth in range(0, division) :\n",
        "      #fitting model over one epoch\n",
        "      train_results = fit_model(model, images[nth*size:(nth+1)*size], time_steps, predictions, batch_size, epoch)\n",
        "\n",
        "      #saving model\n",
        "      stamp = str(epoch-1)+'.'+str(nth+1) if nth < division-1 else str(epoch)\n",
        "      model.save(os.path.join(model_dir, model_name+'_epoch'+stamp+'.h5'))\n",
        "\n",
        "      #testing model\n",
        "      #test_results = test_model(model, time_steps, predictions)\n",
        "\n",
        "      #appending results\n",
        "      result_dir = os.path.join(results_dir, model_name)\n",
        "      if epoch==1 and nth == 0:\n",
        "        results = {}\n",
        "        results['mae_validation']={}\n",
        "        results['mse_validation']={}\n",
        "        #results['mae_test']={}\n",
        "        #results['mse_test']={}\n",
        "        if not os.path.isdir(result_dir):\n",
        "          os.makedirs(result_dir)\n",
        "      else:\n",
        "        with open(os.path.join(result_dir, model_name+'_results.json'), 'r') as fp:\n",
        "          results = json.load(fp)\n",
        "\n",
        "      results['mae_validation'][stamp] = train_results.history['val_mean_absolute_error']\n",
        "      results['mse_validation'][stamp] = train_results.history['val_mean_squared_error']\n",
        "      #results['mae_test'][stamp] = test_results[1]\n",
        "      #results['mse_test'][stamp] = test_results[0]\n",
        "\n",
        "      with open(os.path.join(result_dir, model_name+'_results.json'), 'w') as f:\n",
        "        json.dump(results, f)\n",
        "    model_version += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69roXlQrRFmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "1a577d0a-3af0-47b7-a9b1-f869c27fbfc1"
      },
      "source": [
        "#Using model to predict real disc images\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import PIL.Image as Image\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "  \n",
        "def test_model(test_images, model, time_steps, predictions, window_width):\n",
        "  testing = test_images\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "\n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/ML4NR/models'\n",
        "results_dir = '/content/drive/My Drive/ML4NR/results'\n",
        "data_dir = \"/content/drive/My Drive/ML4NR/disc_data\"\n",
        "\n",
        "model_version = 6001\n",
        "model_epoch = 3\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "time_steps = 21\n",
        "predictions = 1\n",
        "window_width = 360\n",
        "  \n",
        "model_name = 'model'+str(model_version)\n",
        "res_dir = os.path.join(results_dir, model_name)\n",
        "if not os.path.isdir(res_dir):\n",
        "  os.makedirs(res_dir)\n",
        "\n",
        "for model_epoch in range(4, 5):\n",
        "  print('epoch : '+str(model_epoch))\n",
        "  model_weights_name = model_name+'_epoch'+str(model_epoch)+'.h5'\n",
        "  model = load_model(os.path.join(models_dir, model_name, model_weights_name))\n",
        "  \n",
        "  images = os.listdir(data_dir)\n",
        "  for image in images:\n",
        "    if image.endswith('.tif'):\n",
        "      groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "      groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "      groove = normalize_u8bits(groove)\n",
        "\n",
        "      track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "      root = track_data.getroot()\n",
        "      track = []\n",
        "      for x in root.findall('ArrayOfDouble'):\n",
        "        point = []\n",
        "        for d in x.findall('double'):\n",
        "          point.append(int(d.text))\n",
        "        track.append(point)  \n",
        "      track.pop(0)\n",
        "\n",
        "      total_pred = np.empty((len(track), 2))\n",
        "\n",
        "      for i in range(0, len(track), 50000):\n",
        "        print('starting block : '+str(i))\n",
        "        x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "        window=x_data[0]*255 \n",
        "        #print(window.shape)\n",
        "        window=np.squeeze(window)\n",
        "        window = np.array(window)\n",
        "        window = window.astype('uint8')\n",
        "        im = Image.fromarray(window)\n",
        "        im.save(os.path.join(res_dir, image.replace('.tif','_point_'+str(track[i])+'.tif')))\n",
        "        prediction = model.predict(x_data)\n",
        "        prediction = prediction.astype(float)\n",
        "        pred_audio = np.column_stack((prediction, prediction))\n",
        "        for index, line in enumerate(pred_audio):\n",
        "          total_pred[i+index] = line\n",
        "        #print(pred_audio.shape)\n",
        "        #total_pred = np.append(total_pred, pred_audio)\n",
        "        #print(total_pred.shape)\n",
        "      \n",
        "      scipy.io.wavfile.write(\n",
        "          os.path.join(\n",
        "              res_dir, \n",
        "              image+'_model'+str(model_version)+'_epoch'+str(model_epoch)+'_pred.wav'), 104000, total_pred)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 4\n",
            "starting block : 0\n",
            "starting block : 50000\n",
            "starting block : 100000\n",
            "starting block : 150000\n",
            "starting block : 200000\n",
            "starting block : 250000\n",
            "starting block : 300000\n",
            "starting block : 350000\n",
            "starting block : 400000\n",
            "starting block : 450000\n",
            "starting block : 500000\n",
            "starting block : 550000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoxBEB1HA5tL",
        "colab_type": "code",
        "outputId": "edb9b18c-7c8c-41b6-df21-28c44b172633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#transfer learning from model 5000 with real disc images\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#normalize signed 32-bits numpy array to [-1;1]\n",
        "def normalize_32bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+2147483648)*(2))/4294967295)-1\n",
        "  return data\n",
        "\n",
        "#transform one disc image as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions, track, window_width):\n",
        "  reshaped = []\n",
        "  for i in range(0, len(track), predictions):\n",
        "    #print('Window at '+str(track[i][0:2]))\n",
        "    top_bound = max(track[i][0]-time_steps//2, 0)\n",
        "    bottom_bound = min(track[i][0]+time_steps//2 + (1 if time_steps%2==1 else 0), data.shape[0])\n",
        "    left_bound = max(track[i][1]-window_width//2, 0)\n",
        "    right_bound = min(track[i][1]+window_width//2 + (1 if window_width%2==1 else 0), data.shape[1])\n",
        "    window = data[top_bound:bottom_bound, left_bound:right_bound]\n",
        "\n",
        "    padding=((max(0, time_steps//2-track[i][0]), \n",
        "                max(0, track[i][0]+time_steps//2-data.shape[0]+(1 if time_steps%2==1 else 0))),\n",
        "               (max(0, window_width//2-track[i][1]), \n",
        "                max(0, track[i][1]+window_width//2-data.shape[1]+(1 if window_width%2==1 else 0))),\n",
        "                (0,0))\n",
        "    if window.shape[0] < time_steps or window.shape[1] < window_width:\n",
        "      #print('too small : ' + str(window.shape))\n",
        "      #print('adding : '+ str(padding))\n",
        "      window = np.pad(window, padding, 'edge')\n",
        "      #print('result : '+str(padded.shape))\n",
        "      #print(str(padded[:,:10]))\n",
        "      #input()\n",
        "    reshaped.append(window)\n",
        "    \n",
        "  return np.array(reshaped)\n",
        "\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "  \n",
        "def test_model(test_images, model, time_steps, predictions, window_width):\n",
        "  testing = test_images\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "\n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "data_dir = \"/content/drive/My Drive/disc_data\"\n",
        "\n",
        "model_version = 5000\n",
        "model_epoch = 5\n",
        "\n",
        "new_model_version = 5001\n",
        "new_model_epoch_start = 1\n",
        "new_model_epoch_quantity = 5\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "time_steps = 15\n",
        "predictions = 1\n",
        "window_width = 320\n",
        "\n",
        "\n",
        "\n",
        "if new_model_epoch_start == 1:\n",
        "  model_name = 'model'+str(model_version)\n",
        "  model_weights_name = model_name+'_epoch'+str(model_epoch)+'.h5'\n",
        "else:\n",
        "  model_name = 'model'+str(new_model_version)\n",
        "  model_weights_name = model_name+'_epoch'+str(new_model_epoch_start-1)+'.h5'\n",
        "  \n",
        "model_dir = os.path.join(models_dir, model_name)\n",
        "model = load_model(os.path.join(model_dir, model_weights_name))\n",
        "\n",
        "save_model_dir = os.path.join(models_dir, 'model'+str(new_model_version))\n",
        "if not os.path.isdir(save_model_dir):\n",
        "  os.makedirs(save_model_dir)\n",
        "\n",
        "  \n",
        "images = os.listdir(data_dir)\n",
        "for new_model_epoch in range(new_model_epoch_start, new_model_epoch_start+new_model_epoch_quantity):\n",
        "  for image in images:\n",
        "    if not image.endswith('.tif'):\n",
        "      continue\n",
        "    #read image\n",
        "    groove = io.imread(os.path.join(data_dir, image), as_gray=True)\n",
        "    groove = groove.reshape(groove.shape[0], groove.shape[1], 1)\n",
        "    groove = normalize_u8bits(groove)\n",
        "\n",
        "    #read track\n",
        "    track_data = ET.parse(os.path.join(data_dir, image.replace('.tif', '.track')))\n",
        "    root = track_data.getroot()\n",
        "    track = []\n",
        "    for x in root.findall('ArrayOfDouble'):\n",
        "      point = []\n",
        "      for d in x.findall('double'):\n",
        "        point.append(int(d.text))\n",
        "      track.append(point)  \n",
        "    track.pop(0)\n",
        "\n",
        "    #read sound\n",
        "    samplerate, sound=read(os.path.join(data_dir, image.replace('.tif', '.wav')))\n",
        "    sound = np.array(sound[0:len(track)])\n",
        "    sound = normalize_16bits(sound)\n",
        "\n",
        "    for i in range(0, len(track), 50000):\n",
        "      x_data = window_reshape(groove, time_steps, predictions, track[i:min(len(track), i+50000)], window_width)\n",
        "      y_data = sound[i:min(len(track), i+50000)]\n",
        "      model.fit(x_data, y_data, \n",
        "                batch_size=32, \n",
        "                epochs=new_model_epoch+1, \n",
        "                initial_epoch=new_model_epoch, \n",
        "                shuffle=True,\n",
        "                validation_split=0.15,\n",
        "                verbose=1)\n",
        "  model.save(os.path.join(save_model_dir, 'model'+str(new_model_version)+'_epoch'+str(new_model_epoch)+'.h5'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 55s 1ms/step - loss: 0.0247 - mean_absolute_error: 0.1218 - mean_squared_error: 0.0247 - val_loss: 0.0142 - val_mean_absolute_error: 0.0959 - val_mean_squared_error: 0.0142\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0135 - mean_absolute_error: 0.0948 - mean_squared_error: 0.0135 - val_loss: 0.0150 - val_mean_absolute_error: 0.1023 - val_mean_squared_error: 0.0150\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0067 - mean_absolute_error: 0.0652 - mean_squared_error: 0.0067 - val_loss: 0.0029 - val_mean_absolute_error: 0.0452 - val_mean_squared_error: 0.0029\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0227 - mean_absolute_error: 0.1038 - mean_squared_error: 0.0227 - val_loss: 0.1024 - val_mean_absolute_error: 0.2621 - val_mean_squared_error: 0.1024\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0499 - mean_absolute_error: 0.1771 - mean_squared_error: 0.0499 - val_loss: 0.0277 - val_mean_absolute_error: 0.1360 - val_mean_squared_error: 0.0277\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0162 - mean_absolute_error: 0.0875 - mean_squared_error: 0.0162 - val_loss: 0.0691 - val_mean_absolute_error: 0.2010 - val_mean_squared_error: 0.0691\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0459 - mean_absolute_error: 0.1524 - mean_squared_error: 0.0459 - val_loss: 0.0634 - val_mean_absolute_error: 0.1902 - val_mean_squared_error: 0.0634\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 53s 1ms/step - loss: 0.0477 - mean_absolute_error: 0.1731 - mean_squared_error: 0.0477 - val_loss: 0.0320 - val_mean_absolute_error: 0.1467 - val_mean_squared_error: 0.0320\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0214 - mean_absolute_error: 0.1199 - mean_squared_error: 0.0214 - val_loss: 0.0105 - val_mean_absolute_error: 0.0833 - val_mean_squared_error: 0.0105\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0066 - mean_absolute_error: 0.0639 - mean_squared_error: 0.0066 - val_loss: 0.0119 - val_mean_absolute_error: 0.0963 - val_mean_squared_error: 0.0119\n",
            "Train on 42500 samples, validate on 7500 samples\n",
            "Epoch 2/2\n",
            "42500/42500 [==============================] - 54s 1ms/step - loss: 0.0373 - mean_absolute_error: 0.1409 - mean_squared_error: 0.0373 - val_loss: 0.0375 - val_mean_absolute_error: 0.1593 - val_mean_squared_error: 0.0375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-41e3aedb54e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 verbose=1)\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_epoch'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
            "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 29210 input samples and 27287 target samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuwhX5qmsPeS",
        "colab_type": "code",
        "outputId": "98ef8da5-7e6d-4ab9-9ba6-a3700b2cbcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}