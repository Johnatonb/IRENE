{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prototype_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQMBE3qCSf_0",
        "colab_type": "code",
        "outputId": "bf369688-f3c0-46f6-f088-2f21befd836f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Noising a dataset of images and writing in memory\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import PIL.Image as Image\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "\n",
        "\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "\n",
        "def noising(img, seed):\n",
        "    \n",
        "    random.seed(seed)\n",
        "    #scratches generator\n",
        "    for i in range(40000):\n",
        "        h = random.randint(1, 3)\n",
        "        w = random.randint(50, 150)\n",
        "        y = random.randint(0, img.shape[0]+1)\n",
        "        x = random.randint(0, img.shape[1]+1)\n",
        "\n",
        "        img[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "        \n",
        "    #holes generator\n",
        "    for i in range(8000):\n",
        "        h = random.randint(5, 15)\n",
        "        w = random.randint(10, 100)\n",
        "        y = random.randint(0, img.shape[0]+1)\n",
        "        x = random.randint(0, img.shape[1]+1)\n",
        "\n",
        "        img[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "        \n",
        "    np.random.seed(seed)\n",
        "    gauss = np.random.normal(-100.0,50.0,img.shape)\n",
        "    noisy = img + gauss\n",
        "    clipped = np.clip(noisy, 0, 255)\n",
        "    \n",
        "    return np.around(clipped).astype(np.uint8)\n",
        "\n",
        "\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "img_train_dir = \"/content/drive/My Drive/dataset_grooves/images/train\"\n",
        "img_ntrain_dir = \"/content/drive/My Drive/dataset_grooves/images/noisier_train\"\n",
        "img_test_dir = \"/content/drive/My Drive/disc_sim_data/test\"\n",
        "img_ntest_dir = \"/content/drive/My Drive/disc_sim_data/noisy_test\"\n",
        "\n",
        "seed = 8630370\n",
        "\n",
        "if not os.path.isdir(img_ntrain_dir):\n",
        "    os.makedirs(img_ntrain_dir)\n",
        "if not os.path.isdir(img_ntest_dir):\n",
        "    os.makedirs(img_ntest_dir)\n",
        "'''\n",
        "images = os.listdir(img_train_dir)\n",
        "shuffle(images)\n",
        "for image_name in images:\n",
        "    img = io.imread(os.path.join(img_train_dir, image_name) , as_gray=True)\n",
        "    img = noising(img, seed)\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(os.path.join(img_ntrain_dir, image_name))\n",
        "    #input()\n",
        "'''\n",
        "images = os.listdir(img_test_dir)\n",
        "for image_name in images:\n",
        "    img = io.imread(os.path.join(img_test_dir, image_name) , as_gray=True)\n",
        "    img = noising(img, seed)\n",
        "    im = Image.fromarray(img)\n",
        "    im.save(os.path.join(img_ntest_dir, image_name))\n",
        "    #input()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZkF2K8xJlC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating and training model 4004, which has the best results\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from random import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from skimage import io\n",
        "from keras.utils import plot_model\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import scipy.io.wavfile\n",
        "from contextlib import redirect_stdout\n",
        "import json\n",
        "\n",
        "\n",
        "#normalize unsigned 8-bits numpy array to [0;1]\n",
        "def normalize_u8bits(data):\n",
        "  data = data.astype(float)\n",
        "  data /= 255.0\n",
        "  return data\n",
        "\n",
        "#normalize signed 16-bits numpy array to [-1;1]\n",
        "def normalize_16bits(data):\n",
        "  data = data.astype(float)\n",
        "  data = (((data+32768)*(2))/65535)-1\n",
        "  return data\n",
        "\n",
        "#transform one np array as an array of overlapping smaller arrays of height time_steps\n",
        "def window_reshape(data, time_steps, predictions):\n",
        "  reshaped = []\n",
        "  for i in range(0, data.shape[0]+1-time_steps, predictions):\n",
        "    reshaped.append(data[i:i+time_steps,:])\n",
        "  return np.array(reshaped)\n",
        "\n",
        "#generate a perfect sine wave with amplitude range [-1;1] in a format ready for goal\n",
        "def generate_sine(freq, size, offset, predictions):\n",
        "  timestep = 1.0/104000.0\n",
        "  sine = []\n",
        "  block = []\n",
        "  for i in range(size):\n",
        "    amp = math.cos(2*math.pi*freq*(i+offset)*timestep)\n",
        "    block.append(amp)\n",
        "    if (i+1) % predictions == 0:\n",
        "      sine.append(block)\n",
        "      block = []\n",
        "  return np.array(sine)\n",
        "\n",
        "#plot two metrics and save in result folder\n",
        "def plot_and_save(metric_val, metric_test, x_max, title, metric_name, time_step_name):\n",
        "  length = len(metric_train)\n",
        "  plt.plot(range(x_max), metric_val, label=\"validation results\")\n",
        "  plt.plot(range(x_max), metric_test, label=\"test results\")\n",
        "  plt.set_title(title)\n",
        "  plt.set_ylabel(metric_name)\n",
        "  plt.set_xlabel(time_step_name)\n",
        "  plt.legend()\n",
        "  plt.savefig(os.path.join(result_dir, title+'.png'))\n",
        "  plt.close()\n",
        "\n",
        "#create model\n",
        "def create_model(filter_shapes, \n",
        "                 time_steps, \n",
        "                 img_width, \n",
        "                 activation_function, \n",
        "                 max_pool_shape, \n",
        "                 dropout_rate, \n",
        "                 predictions, \n",
        "                 padding,\n",
        "                 learning_rate,\n",
        "                 decay,\n",
        "                 model_dir,\n",
        "                 model_name):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, filter_shapes[0],\n",
        "                   input_shape=(time_steps, img_width, 1), padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(32, filter_shapes[1], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))   \n",
        "  if not dropout_rate[0]==0.0:\n",
        "    model.add(Dropout(dropout_rate[0]))\n",
        "  \n",
        "  model.add(Conv2D(64, filter_shapes[2], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  model.add(Conv2D(64, filter_shapes[3], padding=padding))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not max_pool_shape == (1,1):\n",
        "      model.add(MaxPooling2D(pool_size=max_pool_shape))\n",
        "  if not dropout_rate[1]==0.0:\n",
        "    model.add(Dropout(dropout_rate[1]))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation(activation_function))\n",
        "  if not dropout_rate[2]==0.0:\n",
        "    model.add(Dropout(dropout_rate[2]))\n",
        "  model.add(Dense(predictions))\n",
        "  model.add(Activation('tanh'))\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=opt,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "            \n",
        "  with open(os.path.join(model_dir, model_name+'_parameters.txt'), 'w') as f:\n",
        "    f.write('time_steps_a : '+str(time_steps)+'\\n')\n",
        "    f.write('predictions : ' + str(predictions)+'\\n')\n",
        "    f.write('filter_shapes : '+str(filter_shapes)+'\\n')\n",
        "    f.write('max_pool_shape : '+str(max_pool_shape)+'\\n')\n",
        "    f.write('dropout_rate : '+str(dropout_rate)+'\\n')\n",
        "    f.write('activation : '+activation_function+'\\n')\n",
        "    f.write('padding : '+padding+'\\n')\n",
        "    f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "    f.write('decay : '+str(decay)+'\\n')\n",
        "    f.write('optimizer : '+str(opt)+'\\n')\n",
        "  \n",
        "  model_first_save(model, model_dir, model_name)\n",
        "  return model\n",
        "  \n",
        "def model_first_save(model, model_dir, model_name):\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  plot_model(model, to_file=os.path.join(model_dir, model_name+'_architecture.png'), show_shapes=True, show_layer_names=False)\n",
        "  with open(os.path.join(model_dir, model_name+'_summary.txt'), 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "def fit_model(model, images, time_steps, predictions, batch_size, epoch):\n",
        "  counter=1\n",
        "  for image_name in images:\n",
        "    print('image '+str(counter)+'/'+str(len(images)))\n",
        "    #print('name : '+image_name)\n",
        "    counter+=1\n",
        "    img = io.imread(os.path.join(img_train_dir, image_name) , as_gray=True)\n",
        "    img = noising(img)\n",
        "    img = img.reshape(img.shape[0], img.shape[1], 1)\n",
        "    img_norm = normalize_u8bits(img)\n",
        "    x_train = window_reshape(img_norm, time_steps, predictions)\n",
        "\n",
        "    freq = int(image_name[1:image_name.find('_')])\n",
        "\n",
        "    y_train = generate_sine(freq, x_train.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epoch,\n",
        "                        initial_epoch=epoch-1,\n",
        "                        shuffle=True,\n",
        "                        validation_split=0.15,\n",
        "                        verbose=1)\n",
        "  return history\n",
        "  \n",
        "def test_model(model, time_steps, predictions):\n",
        "  testing = os.listdir(img_test_dir)\n",
        "  i = 0\n",
        "  glob_scores = []\n",
        "  for test in testing:\n",
        "    img_test = io.imread(os.path.join(img_test_dir, test) , as_gray=True)\n",
        "    #img_test = noising(img_test)\n",
        "    img_test = img_test.reshape(img_test.shape[0], img_test.shape[1], 1)\n",
        "    img_test_normalized = normalize_u8bits(img_test)\n",
        "    x_test = window_reshape(img_test_normalized, time_steps, predictions)\n",
        "\n",
        "    freq_test = int(test[1:test.find('_')])\n",
        "    y_test = generate_sine(freq_test, x_test.shape[0]*predictions, (time_steps//2)-(predictions//2), predictions)\n",
        "    \n",
        "    scores = model.evaluate(x_test, y_test)\n",
        "    #print(scores)\n",
        "    glob_scores.append(scores)\n",
        "  means = np.mean(glob_scores, axis=0)\n",
        "  #print(means)\n",
        "  \n",
        "  return means\n",
        "\n",
        "\n",
        "def noising(img):\n",
        "    gauss = np.random.normal(-100.0,50.0,img.shape)\n",
        "    noisy = img + gauss\n",
        "    clipped = np.clip(noisy, 0, 255)\n",
        "    \n",
        "    #scratches generator\n",
        "    for i in range(20000):\n",
        "        h = random.randint(1, 3)\n",
        "        w = random.randint(30, 60)\n",
        "        y = random.randint(0, 80001)\n",
        "        x = random.randint(0, 205)\n",
        "\n",
        "        clipped[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "        \n",
        "    #holes generator\n",
        "    for i in range(4000):\n",
        "        h = random.randint(5, 10)\n",
        "        w = random.randint(10, 30)\n",
        "        y = random.randint(0, 80001)\n",
        "        x = random.randint(0, 205)\n",
        "\n",
        "        clipped[y-h//2:y+h//2, x-w//2:x+w//2] = 0\n",
        "    \n",
        "    return np.around(clipped).astype(np.uint8)\n",
        "  \n",
        "#-----------MAIN----------------\n",
        "#dir paths\n",
        "models_dir = '/content/drive/My Drive/models'\n",
        "results_dir = '/content/drive/My Drive/results'\n",
        "img_train_dir = \"/content/drive/My Drive/dataset_grooves/random_grooves/train\"\n",
        "img_test_dir = \"/content/drive/My Drive/dataset_grooves/random_grooves/noisy_test\"\n",
        "\n",
        "model_version_start = 4004 #for output numbering\n",
        "\n",
        "#network hyper-parameters, default values\n",
        "epoch_start = 4\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "time_steps = 15\n",
        "predictions = 1\n",
        "filter_shapes = [(3,3),(3,3),(3,3),(3,3)]\n",
        "img_width = 220\n",
        "learning_rate = 0.001\n",
        "decay = 0.0\n",
        "steps_for_testing = 100 # 1 step = 1 image\n",
        "max_pool_shape = (2, 2)\n",
        "dropout_rate = [0.8, 0.7, 0.7]\n",
        "activation_function = 'relu'\n",
        "padding='same'\n",
        "division = 10 # epoch division for testing and saving\n",
        "\n",
        "current_epoch = 1\n",
        "current_mv = 0\n",
        "\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  \n",
        "  model_version = model_version_start\n",
        "  \n",
        "  images = os.listdir(img_train_dir)\n",
        "  shuffle(images)\n",
        "  \n",
        "  model_name = 'model'+str(model_version)\n",
        "\n",
        "  print(model_name+', epoch '+str(epoch))\n",
        "  \n",
        "  model_dir = os.path.join(models_dir, model_name)\n",
        "  if not os.path.isdir(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "  #loading model\n",
        "  if epoch > 1:\n",
        "    model = load_model(os.path.join(model_dir, model_name+'_epoch'+str(epoch-1)+'.h5'))\n",
        "  else:\n",
        "\n",
        "    model = create_model(filter_shapes=filter_shapes,\n",
        "                        time_steps=time_steps,\n",
        "                        img_width=img_width,\n",
        "                        activation_function=activation_function,\n",
        "                        max_pool_shape=max_pool_shape,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        predictions=predictions,\n",
        "                        padding=padding,\n",
        "                        learning_rate = learning_rate,\n",
        "                        decay = decay,\n",
        "                        model_dir = model_dir,\n",
        "                        model_name = model_name)\n",
        "  size = len(images)//division\n",
        "  for nth in range(0, division) :\n",
        "    #fitting model over one epoch\n",
        "    train_results = fit_model(model, images[nth*size:(nth+1)*size], time_steps, predictions, batch_size, epoch)\n",
        "\n",
        "    #saving model\n",
        "    stamp = str(epoch-1)+'.'+str(nth+1) if nth < division-1 else str(epoch)\n",
        "    model.save(os.path.join(model_dir, model_name+'_epoch'+stamp+'.h5'))\n",
        "\n",
        "    #testing model\n",
        "    test_results = test_model(model, time_steps, predictions)\n",
        "\n",
        "    #appending results\n",
        "    result_dir = os.path.join(results_dir, model_name)\n",
        "    if epoch==1 and nth == 0:\n",
        "      results = {}\n",
        "      results['mae_validation']={}\n",
        "      results['mse_validation']={}\n",
        "      results['mae_test']={}\n",
        "      results['mse_test']={}\n",
        "      if not os.path.isdir(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "    else:\n",
        "      with open(os.path.join(result_dir, model_name+'_results.json'), 'r') as fp:\n",
        "        results = json.load(fp)\n",
        "\n",
        "    results['mae_validation'][stamp] = train_results.history['val_mean_absolute_error']\n",
        "    results['mse_validation'][stamp] = train_results.history['val_mean_squared_error']\n",
        "    results['mae_test'][stamp] = test_results[1]\n",
        "    results['mse_test'][stamp] = test_results[0]\n",
        "\n",
        "    with open(os.path.join(result_dir, model_name+'_results.json'), 'w') as f:\n",
        "      json.dump(results, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgMgscRvSqu5",
        "colab_type": "code",
        "outputId": "bd518943-3200-4400-d653-065e887e1685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}